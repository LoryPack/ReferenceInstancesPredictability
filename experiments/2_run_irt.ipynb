{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This runs the IRT model on the two datasets once and for all and saves the results in the `data_irt` folder.\n",
    "\n",
    "Note that running this notebook may take long and require a substantial amount of RAM."
   ],
   "id": "67d4f555a390372a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.reference_benchmark import SampleSelector\n",
    "from src.results_loaders import load_reasoning, load_helm_lite\n",
    "\n",
    "\n",
    "def compute_irt(train_df, train_llms, n_embeddings_truncate, irt_file_prefix):\n",
    "    # truncate the embeddings\n",
    "    train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "\n",
    "    # define the selector\n",
    "    selector = SampleSelector(train_df, \"openai_embeddings_subset\", train_llms)\n",
    "\n",
    "    # compute IRT with d=10\n",
    "    selector.compute_IRT_tiny_benchmarks(10, epochs=2000,\n",
    "                                         dataset_name=f'data_irt/{irt_file_prefix}_irtdataset.jsonlines',\n",
    "                                         model_name=f'data_irt/{irt_file_prefix}_irtmodel/')\n"
   ],
   "id": "bc085a7e7dfd9298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n_embeddings_truncate = 1024",
   "id": "c0aea5964278a2e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## KindsOfReasoning",
   "id": "27902d77956fb09e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from src.utils import train_llms_reasoning as train_llms",
   "id": "119f73d4e75cce1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(train_llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings/\")\n",
    "\n",
    "    print(\"reasoning\", split)\n",
    "\n",
    "    compute_irt(train_df, train_llms, n_embeddings_truncate, f\"reasoning_{split}\")"
   ],
   "id": "497f55a691cbe760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HELM-Lite\n",
   "id": "c5c946ed46010ade"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from src.utils import train_llms_helm as train_llms",
   "id": "41c40133e3b6e14d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(train_llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings/\")\n",
    "\n",
    "    print(\"helm OOD LLMs\", split)\n",
    "\n",
    "    compute_irt(train_df, train_llms, n_embeddings_truncate, f\"helm_{split}\")\n"
   ],
   "id": "ffce3cef157b2608",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
