{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Code for training assessors specific to each LLM and evaluating their performance.\n",
    "\n",
    "Note that running this notebook may take long and require a substantial amount of RAM."
   ],
   "id": "762981a252f35f"
  },
  {
   "cell_type": "code",
   "source": [
    "!python --version"
   ],
   "metadata": {
    "id": "d87f5c27dafdc9e2",
    "outputId": "cad5ab6e-5b72-46fa-a54d-6b44fc9331fc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d87f5c27dafdc9e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "source": [
    "from src.classification_utils import _evaluate_predictive_method_from_arrays\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from src.results_loaders import sort_models_order, load_reasoning, load_helm_lite, ngram_vectorize_new, select_features\n",
    "from src.classification_utils import evaluate_predictive_method, predictive_method_list\n",
    "from src.utils import load_with_conditions, save_dataframe\n",
    "\n",
    "# enable reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-30T14:43:05.932748Z",
     "start_time": "2024-08-30T14:43:05.890261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create results folder if it does not exist\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "# create fig folder if it does not exist\n",
    "if not os.path.exists(\"fig\"):\n",
    "    os.makedirs(\"fig\")\n"
   ],
   "id": "d33ec0f9daec0394",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate performance of a classifier for each LLM independently\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "bf6c29fed5aaa54a"
   },
   "id": "bf6c29fed5aaa54a"
  },
  {
   "cell_type": "code",
   "source": [
    "def _check_skip(res_df, pred_method_name, feature_name, split, llm):\n",
    "    if len(res_df)> 0 and len(res_df[(res_df[\"predictive_method\"] == pred_method_name) & (res_df[\"features\"] == feature_name) & (res_df[\"split\"] == split) & (res_df[\"llm\"] == llm)]) > 0:\n",
    "        print(f\"Skipping {split}, {feature_name}, {pred_method_name} for {llm} because it is already in the dataframe\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Doing {split}, {feature_name}, {pred_method_name} for {llm}\")\n",
    "        return False\n",
    "\n",
    "def _concat_and_save(res_df, pred_method_name, feature_name, split, llm,\n",
    "                     BrierScore_val, Calibration_val, Refinement_val, roc_auc_val,\n",
    "                     BrierScore_test, Calibration_test, Refinement_test, roc_auc_test,\n",
    "                     trained_method, filename):\n",
    "    res_df = pd.concat([res_df, pd.DataFrame(\n",
    "    {\"predictive_method\": pred_method_name, \"features\": feature_name, \"split\": split,\n",
    "     \"llm\": llm, \"BrierScore_val\": BrierScore_val, \"Calibration_val\": Calibration_val, \"Refinement_val\": Refinement_val, \"AUROC_val\": roc_auc_val,\n",
    "     \"BrierScore_test\": BrierScore_test, \"Calibration_test\": Calibration_test, \"Refinement_test\": Refinement_test, \"AUROC_test\": roc_auc_test, \"trained_classifier\": trained_method}, index=[0])])\n",
    "\n",
    "    save_dataframe(filename, res_df)\n",
    "    return res_df\n",
    "\n",
    "def evaluate_and_update(res_df, train_df, validation_df, test_df, features, predictive_method,\n",
    "                        pred_method_name, feature_name, split, llm,\n",
    "                        filename, **kwargs):\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name, split, llm):\n",
    "        BrierScore_test, Calibration_test, Refinement_test, roc_auc_test, trained_method = evaluate_predictive_method(train_df, test_df,\n",
    "                                                                                                  features,\n",
    "                                                                                                  f\"Success_{llm}\",\n",
    "                                                                                                  predictive_method=predictive_method,\n",
    "                                                                                                  return_trained_method=True,\n",
    "                                                                                                  **kwargs)\n",
    "        BrierScore_val, Calibration_val, Refinement_val, roc_auc_val = evaluate_predictive_method(train_df, validation_df,\n",
    "                                                                                                  features,\n",
    "                                                                                                  f\"Success_{llm}\",\n",
    "                                                                                                  predictive_method=predictive_method,\n",
    "                                                                                                  trained_method=trained_method,\n",
    "                                                                                                  **kwargs)\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, split, llm,\n",
    "                                  BrierScore_val, Calibration_val, Refinement_val, roc_auc_val,\n",
    "                                  BrierScore_test, Calibration_test, Refinement_test, roc_auc_test,\n",
    "                                  trained_method, filename)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def evaluate_and_update_arrays(res_df,  X_train, train_labels, X_val, val_labels,\n",
    "                               X_test, test_labels, predictive_method,\n",
    "                               pred_method_name, feature_name, split, llm,\n",
    "                               filename,\n",
    "                               **kwargs):\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name, split, llm):\n",
    "        BrierScore_test, Calibration_test, Refinement_test, roc_auc_test, trained_method = _evaluate_predictive_method_from_arrays(X_train,\n",
    "                                                                                                               train_labels,\n",
    "                                                                                                               X_test,\n",
    "                                                                                                               test_labels,\n",
    "                                                                                                               predictive_method=predictive_method,\n",
    "                                                                                                               return_trained_method=True,\n",
    "                                                                                                               **kwargs)\n",
    "        BrierScore_val, Calibration_val, Refinement_val, roc_auc_val = _evaluate_predictive_method_from_arrays(X_train,\n",
    "                                                                                                               train_labels,\n",
    "                                                                                                               X_val,\n",
    "                                                                                                               val_labels,\n",
    "                                                                                                               predictive_method=predictive_method,\n",
    "                                                                                                               trained_method=trained_method,\n",
    "                                                                                                               **kwargs)\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, split, llm,\n",
    "                                  BrierScore_val, Calibration_val, Refinement_val, roc_auc_val,\n",
    "                                  BrierScore_test, Calibration_test, Refinement_test, roc_auc_test,\n",
    "                                  trained_method, filename)\n",
    "    return res_df"
   ],
   "metadata": {
    "id": "97a86ba65a809690"
   },
   "id": "97a86ba65a809690",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_best_predictive_method_per_feature(all_datasets_assessors, sort=False, set_axis_bounds=True, metric=\"AUROC_test\", yaxixlabel=None, col_order=None, font_scale=1, legend_text_size=None, height=5, aspect=1, legend_loc=(1.03, 0.6), panel_space=None):\n",
    "    # Extract best method based on validation split\n",
    "    best_predictive_method_per_feature = all_datasets_assessors.groupby([\"llm\", \"features\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)\n",
    "    \n",
    "    # Sort using the order of the models\n",
    "    if sort:\n",
    "        best_predictive_method_per_feature = best_predictive_method_per_feature.sort_values(by=[\"llm\"], key=lambda x: x.apply(lambda y: sort_models_order.index(y)))\n",
    "    else:\n",
    "        best_predictive_method_per_feature = best_predictive_method_per_feature.sort_values(by=[\"llm\"])\n",
    "    \n",
    "    best_predictive_method_per_feature[\"split\"] = best_predictive_method_per_feature[\"split\"].astype(str).replace(\"False\", \"In distribution\").replace(\"OOD_1\", \"OOD 1\").replace(\"OOD_2\", \"OOD 2\").replace(\"OOD_3\", \"OOD 3\").replace(\"OOD_4\", \"OOD 4\")\n",
    "\n",
    "    # Plot using seaborn\n",
    "    # Adjust font scale\n",
    "    sns.set_context(\"notebook\", font_scale=font_scale)\n",
    "    \n",
    "    catplot = sns.catplot(\n",
    "        data=best_predictive_method_per_feature,\n",
    "        x='llm', y=metric, hue='features', col=\"split\",\n",
    "        kind='bar', hue_order=[\"openai\", \"fasttext\", \"ngrams_1\", \"word2vec\"],\n",
    "        col_order=col_order, \n",
    "        height=height, aspect=aspect\n",
    "    )\n",
    "\n",
    "    # Rotate x labels\n",
    "    for i, ax in enumerate(plt.gcf().axes):\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        if i == 0 and yaxixlabel is not None:\n",
    "            ax.set_ylabel(yaxixlabel)\n",
    "    \n",
    "    catplot.fig.subplots_adjust(wspace=panel_space)\n",
    "    \n",
    "    # Adjust legend font size\n",
    "    catplot._legend.set_title(\"Features\")\n",
    "    if legend_text_size is not None:\n",
    "        plt.setp(catplot._legend.get_texts(), fontsize=f'{legend_text_size}')  # for legend text\n",
    "        plt.setp(catplot._legend.get_title(), fontsize=f'{legend_text_size+1}')  # for legend title\n",
    "\n",
    "    # Move legend to an appropriate position\n",
    "    catplot._legend.set_bbox_to_anchor(legend_loc)\n",
    "    catplot._legend.set_frame_on(False)  # Optionally, remove the legend frame for better appearance\n",
    "\n",
    "    # Set axis bounds\n",
    "    if set_axis_bounds:\n",
    "        for ax in plt.gcf().axes:\n",
    "            ax.set_ylim(0.5, 1)"
   ],
   "metadata": {
    "id": "d7a40bf15b808bbf"
   },
   "id": "d7a40bf15b808bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KindsOfReasoning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "becd50ffbc71dd44"
   },
   "id": "becd50ffbc71dd44"
  },
  {
   "cell_type": "code",
   "source": "filename = \"results/specific_assessors_reasoning.pkl\"",
   "metadata": {
    "id": "d413a00c0ca47c46"
   },
   "id": "d413a00c0ca47c46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "all_datasets_assessors = load_with_conditions(filename, overwrite_res)"
   ],
   "metadata": {
    "id": "929c347ab826d9d5",
    "outputId": "04aa02a5-a8ae-4e0d-ec5a-f3ef83e53ffc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "929c347ab826d9d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llms = [\n",
    "    'text-ada-001',\n",
    "    'text-babbage-001',\n",
    "    'text-curie-001',\n",
    "    'text-davinci-001',\n",
    "    'text-davinci-002',\n",
    "    'text-davinci-003',\n",
    "    'gpt-3.5-turbo-0301',\n",
    "    'gpt-3.5-turbo-0613',\n",
    "    'gpt-3.5-turbo-1106',\n",
    "    'gpt-3.5-turbo-0125',\n",
    "    'gpt-4-0314',\n",
    "    'gpt-4-0613',\n",
    "    'gpt-4-1106-preview',\n",
    "    'gpt-4-0125-preview',\n",
    "]"
   ],
   "metadata": {
    "id": "5ee773d095771fe6"
   },
   "id": "5ee773d095771fe6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split into different cells for OpenAI embeddings, word2vec, fasttext and n-grams."
   ],
   "metadata": {
    "collapsed": false,
    "id": "139f470a1fc0c600"
   },
   "id": "139f470a1fc0c600"
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenAI embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4a7e5a0dd6157e1d"
   },
   "id": "4a7e5a0dd6157e1d"
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings/\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "            all_datasets_assessors = evaluate_and_update(all_datasets_assessors, train_df, validation_df, test_df, [\"openai_embeddings_large\"], predictive_method, pred_method_name, \"openai\", split, llm, filename, **kwargs)\n"
   ],
   "metadata": {
    "id": "3482058103e311c5"
   },
   "id": "3482058103e311c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data with embeddings does not have the sampling column which is needed to extract the system prompt; do not remove it anymore and re-compute the embeddings!"
   ],
   "metadata": {
    "collapsed": false,
    "id": "47848e255f500c36"
   },
   "id": "47848e255f500c36"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word2vec + fasttext"
   ],
   "metadata": {
    "collapsed": false,
    "id": "2c287074bd328f5e"
   },
   "id": "2c287074bd328f5e"
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"word2vec\", \"fasttext\"], ood_split=split, base_path=\"../results/kindsofreasoning/\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for embedding_type in [\"word2vec\", \"fasttext\"]:\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                all_datasets_assessors = evaluate_and_update(all_datasets_assessors, train_df, validation_df, test_df, [f\"{embedding_type}_embeddings\"], predictive_method, pred_method_name, embedding_type, split, llm, filename, **kwargs)\n"
   ],
   "metadata": {
    "id": "35ac35d7fa6758e0"
   },
   "id": "35ac35d7fa6758e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "ngrams"
   ],
   "metadata": {
    "collapsed": false,
    "id": "e59a85ba8c280ca"
   },
   "id": "e59a85ba8c280ca"
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [], ood_split=split, base_path=\"../results/kindsofreasoning/\")\n",
    "\n",
    "    for n_gram_size in [1]:\n",
    "        # compute the n-grams\n",
    "        X_train_ngrams, X_val_ngrams, X_test_ngrams, vectorizer = ngram_vectorize_new(train_df[\"prompt\"], validation_df[\"prompt\"], test_df[\"prompt\"], ngram_range=(1, n_gram_size))\n",
    "\n",
    "        for llm in llms:\n",
    "            if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "                continue\n",
    "\n",
    "            if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "                continue\n",
    "\n",
    "            if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "                continue\n",
    "\n",
    "            # select the features (this depends on which LLM you are considering)\n",
    "            X_train_ngrams_selected, X_val_ngrams_selected, X_test_ngrams_selected, selector = select_features(X_train_ngrams, train_df[f\"Success_{llm}\"], X_val_ngrams, X_test_ngrams)\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                all_datasets_assessors = evaluate_and_update_arrays(all_datasets_assessors, X_train_ngrams_selected, train_df[f\"Success_{llm}\"], X_val_ngrams_selected, validation_df[f\"Success_{llm}\"], X_test_ngrams_selected, test_df[f\"Success_{llm}\"], predictive_method, pred_method_name, f\"ngrams_{n_gram_size}\", split, llm, filename, **kwargs)"
   ],
   "metadata": {
    "id": "e7a6617b9d67cb93"
   },
   "id": "e7a6617b9d67cb93",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "all_datasets_assessors"
   ],
   "metadata": {
    "id": "9a12d16f4377838c",
    "outputId": "8b49f907-d585-414b-ecd4-cd71cef5a19c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    }
   },
   "id": "9a12d16f4377838c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "all_datasets_assessors.split.unique()"
   ],
   "metadata": {
    "id": "8ccfdea2c6ac49e5",
    "outputId": "d298048c-8786-4379-8c58-08fd8ec29370"
   },
   "id": "8ccfdea2c6ac49e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plots\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cf828eaf4a6b87b6"
   },
   "id": "cf828eaf4a6b87b6"
  },
  {
   "cell_type": "code",
   "source": [
    "filename = \"results/specific_assessors_reasoning.pkl\"\n",
    "all_datasets_assessors = load_with_conditions(filename)"
   ],
   "metadata": {
    "id": "452cb6eba2df28de",
    "outputId": "53c6497e-e21e-4a83-b4de-7121d425b93b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "452cb6eba2df28de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors, sort=True, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], metric=\"AUROC_val\", legend_text_size=24, aspect=0.9, font_scale=2, panel_space=0.1, legend_loc=(0.99, 0.6))\n",
    "plt.savefig(\"fig/reasoning_results_full_dataset_val.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "fff2a2b53300e24c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors, sort=True, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], metric=\"AUROC_test\", legend_text_size=24, aspect=0.9, font_scale=2, panel_space=0.1, legend_loc=(0.99, 0.6))\n",
    "plt.savefig(\"fig/reasoning_results_full_dataset_test.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "26de6277e7b3f82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8b316d3e3ac9c193"
   },
   "cell_type": "markdown",
   "source": [
    "Compute the number of times each feature \"wins\" over the others, for each split separately."
   ],
   "id": "8b316d3e3ac9c193"
  },
  {
   "metadata": {
    "id": "8d497c805349ae7f",
    "outputId": "36e02d1a-329c-402c-947c-fe138e329865"
   },
   "cell_type": "code",
   "source": [
    "# find the best over feature and predictive method, for each llm and split\n",
    "best_over_feat_predictive_method = all_datasets_assessors.groupby([\"llm\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)\n",
    "# then for each split separately, count the number of times each feature \"wins\" over the others (ie if it is present in the best_over_feat_predictive_method)\n",
    "for split in best_over_feat_predictive_method.split.unique():\n",
    "    # for each feature, count the number of times it is the best\n",
    "    counts_split = best_over_feat_predictive_method[(best_over_feat_predictive_method.split == split)].features.value_counts()\n",
    "    # normalize by the number of llms\n",
    "    counts_split = counts_split / sum(counts_split)\n",
    "    print(f\"Split {split}:\")\n",
    "    print(counts_split)"
   ],
   "id": "8d497c805349ae7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1fe322882b1debff"
   },
   "cell_type": "markdown",
   "source": [
    "Here, OpenAI embeddings win more frequently."
   ],
   "id": "1fe322882b1debff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HELM-Lite"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d23d7a3c74d585a9"
   },
   "id": "d23d7a3c74d585a9"
  },
  {
   "cell_type": "code",
   "source": "filename = \"results/specific_assessors_helm.pkl\"",
   "metadata": {
    "id": "bf3deb7a39d68123"
   },
   "id": "bf3deb7a39d68123",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "all_datasets_assessors = load_with_conditions(filename, overwrite_res)"
   ],
   "metadata": {
    "id": "5eedc71aa668ac98",
    "outputId": "4a9139ef-e4df-4381-850f-a1cbf2cfff36",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "5eedc71aa668ac98",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llms_dict = {'01-ai/yi-34b': '01-ai/yi-34b',\n",
    "             '01-ai/yi-6b': '01-ai/yi-6b',\n",
    "             'AlephAlpha/luminous-base': 'AlephAlpha/luminous-base',\n",
    "             'AlephAlpha/luminous-extended': 'AlephAlpha/luminous-extended',\n",
    "             'AlephAlpha/luminous-supreme': 'AlephAlpha/luminous-supreme',\n",
    "             'ai21/j2-grande': 'ai21/j2-grande',\n",
    "             'ai21/j2-jumbo': 'ai21/j2-jumbo',\n",
    "             'anthropic/claude-2.0': 'anthropic/claude-2.0',\n",
    "             'anthropic/claude-2.1': 'anthropic/claude-2.1',\n",
    "             'anthropic/claude-instant-1.2': 'anthropic/claude-instant-1.2',\n",
    "             'anthropic/claude-v1.3': 'anthropic/claude-v1.3',\n",
    "             'cohere/command': 'cohere/command',\n",
    "             'cohere/command-light': 'cohere/command-light',\n",
    "             'google/text-bison@001': 'google/text-bison@001',\n",
    "             'google/text-unicorn@001': 'google/text-unicorn@001',\n",
    "             'meta/llama-2-13b': 'meta/llama-2-13b',\n",
    "             'meta/llama-2-70b': 'meta/llama-2-70b',\n",
    "             'meta/llama-2-7b': 'meta/llama-2-7b',\n",
    "             'meta/llama-65b': 'meta/llama-65b',\n",
    "             'mistralai/mistral-7b-v0.1': 'mistralai/mistral-7b-v0.1',\n",
    "             'mistralai/mixtral-8x7b-32kseqlen': 'mistralai/mixtral-8x7b-32kseqlen',\n",
    "             'gpt-3.5-turbo-0613': 'openai/gpt-3.5-turbo-0613',\n",
    "             'gpt-4-0613': 'openai/gpt-4-0613',\n",
    "             'gpt-4-1106-preview': 'openai/gpt-4-1106-preview',\n",
    "             'text-davinci-002': 'openai/text-davinci-002',\n",
    "             'text-davinci-003': 'openai/text-davinci-003',\n",
    "             'tiiuae/falcon-40b': 'tiiuae/falcon-40b',\n",
    "             'tiiuae/falcon-7b': 'tiiuae/falcon-7b',\n",
    "             'writer/palmyra-x-v2': 'writer/palmyra-x-v2',\n",
    "             'writer/palmyra-x-v3': 'writer/palmyra-x-v3'}\n",
    "llms = list(llms_dict.keys())"
   ],
   "metadata": {
    "id": "8d5ebc3914267a50"
   },
   "id": "8d5ebc3914267a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split into different cells for OpenAI embeddings, word2vec, fasttext and n-grams (to avoid memory issues)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "2b66ef4c9b9d1375"
   },
   "id": "2b66ef4c9b9d1375"
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenAI embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "id": "caaccefba586ce40"
   },
   "id": "caaccefba586ce40"
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings/\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "            all_datasets_assessors = evaluate_and_update(all_datasets_assessors, train_df, validation_df, test_df, [\"openai_embeddings_large\"], predictive_method, pred_method_name, \"openai\", split, llm, filename, **kwargs)"
   ],
   "metadata": {
    "id": "d65ed1d5b6aa54fe"
   },
   "id": "d65ed1d5b6aa54fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word2vec + fasttext"
   ],
   "metadata": {
    "collapsed": false,
    "id": "69b41ae7cc870be"
   },
   "id": "69b41ae7cc870be"
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"word2vec\", \"fasttext\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0/\")\n",
    "\n",
    "    print(f'train_df samples: {train_df.shape[0]}')\n",
    "    print(f'test_df samples: {test_df.shape[0]}')\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for embedding_type in [\"word2vec\", \"fasttext\"]:\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                all_datasets_assessors = evaluate_and_update(all_datasets_assessors, train_df, validation_df,  test_df, [f\"{embedding_type}_embeddings\"], predictive_method, pred_method_name, embedding_type, split, llm, filename, **kwargs)\n"
   ],
   "metadata": {
    "id": "442f578aed85e39c"
   },
   "id": "442f578aed85e39c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "ngrams"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ef0cb90e1ccca9a1"
   },
   "id": "ef0cb90e1ccca9a1"
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [], ood_split=split, base_path=\"../results/helm_lite_v1.0.0/\")\n",
    "\n",
    "    for n_gram_size in [1]:\n",
    "        # compute the n-grams\n",
    "        X_train_ngrams, X_val_ngrams, X_test_ngrams, vectorizer = ngram_vectorize_new(train_df[\"prompt\"], validation_df[\"prompt\"], test_df[\"prompt\"], ngram_range=(1, n_gram_size))\n",
    "\n",
    "        for llm in llms:\n",
    "            if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "                continue\n",
    "\n",
    "            if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "                continue\n",
    "\n",
    "            if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "                continue\n",
    "\n",
    "            # select the features (this depends on which LLM you are considering)\n",
    "            X_train_ngrams_selected, X_val_ngrams_selected, X_test_ngrams_selected, selector = select_features(X_train_ngrams, train_df[f\"Success_{llm}\"], X_val_ngrams, X_test_ngrams)\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                all_datasets_assessors = evaluate_and_update_arrays(all_datasets_assessors, X_train_ngrams_selected, train_df[f\"Success_{llm}\"], X_val_ngrams_selected, validation_df[f\"Success_{llm}\"], X_test_ngrams_selected, test_df[f\"Success_{llm}\"], predictive_method, pred_method_name, f\"ngrams_{n_gram_size}\", split, llm, filename, **kwargs)"
   ],
   "metadata": {
    "id": "d7a05d0dc33ba61a"
   },
   "id": "d7a05d0dc33ba61a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plots\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4368b8e896ee545"
   },
   "id": "4368b8e896ee545"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filename = \"results/specific_assessors_helm.pkl\"",
   "id": "d79cdca867cc8400",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "all_datasets_assessors = load_with_conditions(filename)"
   ],
   "metadata": {
    "id": "fceb5dd735472be1",
    "outputId": "70cc9e84-a420-427d-ef93-144cbcee8ef1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "fceb5dd735472be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(all_datasets_assessors.llm.unique())"
   ],
   "metadata": {
    "id": "52a01aee16093fb5",
    "outputId": "cdeb803b-7bb1-440b-a023-2333e7e36634"
   },
   "id": "52a01aee16093fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors, sort=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\"], metric=\"AUROC_val\", legend_text_size=24, aspect=1.2, font_scale=1.3, panel_space=0.1, legend_loc=(1.01, 0.5))\n",
    "plt.savefig(\"fig/helm_results_full_dataset_val.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "9c8de3936dee9cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors, sort=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\"], metric=\"AUROC_test\", legend_text_size=24, aspect=1.2, font_scale=1.3, panel_space=0.1, legend_loc=(1.01, 0.5))\n",
    "plt.savefig(\"fig/helm_results_full_dataset_test.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "2791e68a96f177fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c03fb33d05d772e"
   },
   "cell_type": "markdown",
   "source": [
    "Compute the number of times each feature \"wins\" over the others, for each split separately."
   ],
   "id": "c03fb33d05d772e"
  },
  {
   "metadata": {
    "id": "ab8e1a5939cf1f98",
    "outputId": "28e91383-1be0-40e9-e678-419197d4ee5c"
   },
   "cell_type": "code",
   "source": [
    "# find the best over feature and predictive method, for each llm and split\n",
    "best_over_feat_predictive_method = all_datasets_assessors.groupby([\"llm\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)\n",
    "# then for each split separately, count the number of times each feature \"wins\" over the others (ie if it is present in the best_over_feat_predictive_method)\n",
    "for split in best_over_feat_predictive_method.split.unique():\n",
    "    # for each feature, count the number of times it is the best\n",
    "    counts_split = best_over_feat_predictive_method[(best_over_feat_predictive_method.split == split)].features.value_counts()\n",
    "    # normalize by the number of llms\n",
    "    counts_split = counts_split / sum(counts_split)\n",
    "    print(f\"Split {split}:\")\n",
    "    print(counts_split)"
   ],
   "id": "ab8e1a5939cf1f98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "dadebbea30918481"
   },
   "cell_type": "markdown",
   "source": [
    "Here instead I get a much more mixed picture, with fasttext dominating ood_2, while openai embeddings are still the top or very close in the other 3 cases."
   ],
   "id": "dadebbea30918481"
  },
  {
   "metadata": {
    "id": "663bbf18887c9571"
   },
   "cell_type": "markdown",
   "source": [
    "### KindsOfReasoning subsampled to have same samples size as HELM-Lite\n",
    "\n",
    "This to check whether the better performance on reasoning is due to the larger sample size. Reduce to:\n",
    "\n",
    "train_df samples: 3000\n",
    "\n",
    "test_df samples: 1285\n",
    "\n",
    "In principle I should not reduce the test set as well. Maybe re-run without doing so."
   ],
   "id": "663bbf18887c9571"
  },
  {
   "metadata": {
    "id": "d2ba0161790e0790"
   },
   "cell_type": "code",
   "source": [
    "n_train = 3000\n",
    "n_test = 1285"
   ],
   "id": "d2ba0161790e0790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6f7e1444fd570a50"
   },
   "cell_type": "code",
   "source": "filename = \"results/specific_assessors_reasoning_subsampled.pkl\"",
   "id": "6f7e1444fd570a50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "eeb6c45255dc4705",
    "outputId": "0fbe9f55-f9aa-4ec8-dea1-4e92e7f18b6e"
   },
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "all_datasets_assessors_subsampled = load_with_conditions(filename, overwrite_res)"
   ],
   "id": "eeb6c45255dc4705",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9a7e78800786725"
   },
   "cell_type": "code",
   "source": [
    "llms = [\n",
    "    'text-ada-001',\n",
    "    'text-babbage-001',\n",
    "    'text-curie-001',\n",
    "    'text-davinci-001',\n",
    "    'text-davinci-002',\n",
    "    'text-davinci-003',\n",
    "    'gpt-3.5-turbo-0301',\n",
    "    'gpt-3.5-turbo-0613',\n",
    "    'gpt-3.5-turbo-1106',\n",
    "    'gpt-3.5-turbo-0125',\n",
    "    'gpt-4-0314',\n",
    "    'gpt-4-0613',\n",
    "    'gpt-4-1106-preview',\n",
    "    'gpt-4-0125-preview',\n",
    "]"
   ],
   "id": "9a7e78800786725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "98b302cf89f3bcc4"
   },
   "cell_type": "markdown",
   "source": [
    "Split into different cells for OpenAI embeddings, word2vec, fasttext and n-grams."
   ],
   "id": "98b302cf89f3bcc4"
  },
  {
   "metadata": {
    "id": "ba1a06662b2ef909"
   },
   "cell_type": "markdown",
   "source": [
    "OpenAI embeddings"
   ],
   "id": "ba1a06662b2ef909"
  },
  {
   "metadata": {
    "id": "81748cc83ac34b4f"
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, subsampled_n_train=3000, \n",
    "                                                      base_path=\"../results/kindsofreasoning_embeddings\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "            all_datasets_assessors_subsampled = evaluate_and_update(all_datasets_assessors_subsampled, train_df, validation_df, test_df, [\"openai_embeddings_large\"], predictive_method, pred_method_name, \"openai\", split, llm, filename, **kwargs)\n"
   ],
   "id": "81748cc83ac34b4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6e15de7df0421e0e"
   },
   "cell_type": "markdown",
   "source": [
    "The data with embeddings does not have the sampling column which is needed to extract the system prompt; do not remove it anymore and re-compute the embeddings!"
   ],
   "id": "6e15de7df0421e0e"
  },
  {
   "metadata": {
    "id": "5694e21182654652"
   },
   "cell_type": "markdown",
   "source": [
    "Word2vec + fasttext"
   ],
   "id": "5694e21182654652"
  },
  {
   "metadata": {
    "id": "2c4009980ed6a5e6"
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"word2vec\", \"fasttext\"], ood_split=split, subsampled_n_train=3000, \n",
    "                                                      base_path=\"../results/kindsofreasoning/\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for embedding_type in [\"word2vec\", \"fasttext\"]:\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                all_datasets_assessors_subsampled = evaluate_and_update(all_datasets_assessors_subsampled, train_df, validation_df, test_df, [f\"{embedding_type}_embeddings\"], predictive_method, pred_method_name, embedding_type, split, llm, filename, **kwargs)\n"
   ],
   "id": "2c4009980ed6a5e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d6ef88bf4f6f464a"
   },
   "cell_type": "markdown",
   "source": [
    "ngrams"
   ],
   "id": "d6ef88bf4f6f464a"
  },
  {
   "metadata": {
    "id": "ea946f290eace20d"
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [], ood_split=split, subsampled_n_train=3000, \n",
    "                                                      base_path=\"../results/kindsofreasoning/\")\n",
    "\n",
    "    for n_gram_size in [1]:\n",
    "        # compute the n-grams\n",
    "        X_train_ngrams, X_val_ngrams, X_test_ngrams, vectorizer = ngram_vectorize_new(train_df[\"prompt\"], validation_df[\"prompt\"], test_df[\"prompt\"], ngram_range=(1, n_gram_size))\n",
    "\n",
    "        for llm in llms:\n",
    "            if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "                continue\n",
    "\n",
    "            if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "                continue\n",
    "\n",
    "            if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "                print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "                continue\n",
    "\n",
    "            # select the features (this depends on which LLM you are considering)\n",
    "            X_train_ngrams_selected, X_val_ngrams_selected, X_test_ngrams_selected, selector = select_features(X_train_ngrams, train_df[f\"Success_{llm}\"], X_val_ngrams, X_test_ngrams)\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                all_datasets_assessors_subsampled = evaluate_and_update_arrays(all_datasets_assessors_subsampled, X_train_ngrams_selected, train_df[f\"Success_{llm}\"], X_val_ngrams_selected, validation_df[f\"Success_{llm}\"], X_test_ngrams_selected, test_df[f\"Success_{llm}\"], predictive_method, pred_method_name, f\"ngrams_{n_gram_size}\", split, llm, filename, **kwargs)"
   ],
   "id": "ea946f290eace20d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4b71b1ae76f06f20",
    "outputId": "4b9d6ab1-d1e9-47e9-ed07-1756644fe653"
   },
   "cell_type": "code",
   "source": [
    "len(all_datasets_assessors_subsampled)"
   ],
   "id": "4b71b1ae76f06f20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5fc340a549e41ff3",
    "outputId": "8bb0af6e-6abf-4210-bf5b-8fa53f074fe7"
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors_subsampled.split.unique()"
   ],
   "id": "5fc340a549e41ff3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "42d0bc904d156741"
   },
   "cell_type": "markdown",
   "source": [
    "#### Plots\n",
    "\n",
    "This is for the subsampled setup."
   ],
   "id": "42d0bc904d156741"
  },
  {
   "metadata": {
    "id": "15ff5f665b2cfff7",
    "outputId": "df9f3343-9242-4e05-a556-165b07e8f1ff"
   },
   "cell_type": "code",
   "source": [
    "filename = \"results/specific_assessors_reasoning_subsampled.pkl\"\n",
    "all_datasets_assessors_subsampled = load_with_conditions(filename)"
   ],
   "id": "15ff5f665b2cfff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessors_subsampled.llm.unique()",
   "id": "7e868c2c8e6ab27f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ffe70842b8391809",
    "outputId": "af481a69-34b9-4c3e-8938-c9c6d581953f"
   },
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors_subsampled, sort=True, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], metric=\"AUROC_val\", legend_text_size=24, aspect=0.9, font_scale=2, panel_space=0.1, legend_loc=(0.99, 0.6))\n",
    "plt.savefig(\"fig/reasoning_results_subsampled_dataset_val.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "ffe70842b8391809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors_subsampled, sort=True, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], metric=\"AUROC_test\", legend_text_size=24, aspect=0.9, font_scale=2, panel_space=0.1, legend_loc=(0.99, 0.6))\n",
    "plt.savefig(\"fig/reasoning_results_subsampled_dataset_test.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "56d88c2c6f0a23e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "dab3a5da1e258387"
   },
   "cell_type": "markdown",
   "source": [
    "Compute the number of times each feature \"wins\" over the others, for each split separately."
   ],
   "id": "dab3a5da1e258387"
  },
  {
   "metadata": {
    "id": "1c3c7bb2a865fce8",
    "outputId": "ef285ba2-0632-485a-a405-7d579699dfb0"
   },
   "cell_type": "code",
   "source": [
    "# find the best over feature and predictive method, for each llm and split\n",
    "best_over_feat_predictive_method = all_datasets_assessors_subsampled.groupby([\"llm\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)\n",
    "# then for each split separately, count the number of times each feature \"wins\" over the others (ie if it is present in the best_over_feat_predictive_method)\n",
    "for split in best_over_feat_predictive_method.split.unique():\n",
    "    # for each feature, count the number of times it is the best\n",
    "    counts_split = best_over_feat_predictive_method[(best_over_feat_predictive_method.split == split)].features.value_counts()\n",
    "    # normalize by the number of llms\n",
    "    counts_split = counts_split / sum(counts_split)\n",
    "    print(f\"Split {split}:\")\n",
    "    print(counts_split)"
   ],
   "id": "1c3c7bb2a865fce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "52ccea4e7cf0ee6c"
   },
   "cell_type": "markdown",
   "source": [
    "Results do not seem to change much. The main difference is that the OpenAI embedding win less frequently than before.\n",
    "#### Now compute the pairwise difference between the full data and the subsampled setup."
   ],
   "id": "52ccea4e7cf0ee6c"
  },
  {
   "metadata": {
    "id": "9fa54e2d674aae40",
    "outputId": "8b70eef4-7997-4ae0-d9a8-5abae16da45a"
   },
   "cell_type": "code",
   "source": [
    "filename = \"results/specific_assessors_reasoning_subsampled.pkl\"\n",
    "all_datasets_assessors_subsampled = load_with_conditions(filename)"
   ],
   "id": "9fa54e2d674aae40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cb68962154acda3f",
    "outputId": "75c584d3-3eef-4ab8-d133-7569ba1e09da"
   },
   "cell_type": "code",
   "source": [
    "filename = \"results/specific_assessors_reasoning.pkl\"\n",
    "all_datasets_assessors = load_with_conditions(filename)"
   ],
   "id": "cb68962154acda3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e950388284faa95d"
   },
   "cell_type": "code",
   "source": [
    "# select the best predictive method for each feature, llm and split\n",
    "all_datasets_assessors_subsampled = all_datasets_assessors_subsampled.groupby([\"llm\", \"features\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)\n",
    "all_datasets_assessors = all_datasets_assessors.groupby([\"llm\", \"features\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)"
   ],
   "id": "e950388284faa95d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e5f0f67a9668e5e6"
   },
   "cell_type": "code",
   "source": [
    "# merge the subsampled on the full one using [\"llm\", \"features\", \"split\", \"predictive_method\"]\n",
    "all_datasets_assessors = pd.merge(all_datasets_assessors, all_datasets_assessors_subsampled, on=[\"llm\", \"features\", \"split\"], suffixes=(\"\", \"_subsampled\"))\n",
    "# compute difference in AUROC\n",
    "all_datasets_assessors[\"AUROC_test_diff\"] = all_datasets_assessors[\"AUROC_test\"] - all_datasets_assessors[\"AUROC_test_subsampled\"]\n",
    "all_datasets_assessors[\"AUROC_val_diff\"] = all_datasets_assessors[\"AUROC_val\"] - all_datasets_assessors[\"AUROC_val_subsampled\"]"
   ],
   "id": "e5f0f67a9668e5e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9aa9b8ca1622f528",
    "outputId": "d353fc29-1a2e-4ab9-dbcd-4af0b1c89616"
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors.shape"
   ],
   "id": "9aa9b8ca1622f528",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b93084f9a6327c00",
    "outputId": "344d6ed8-607c-40f9-a5ba-102fce4d6390"
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors.columns"
   ],
   "id": "b93084f9a6327c00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "aa1473f2436c40",
    "outputId": "b538dcb2-064d-4ef9-fa7c-cc805afb796a"
   },
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors, sort=True, set_axis_bounds=False, metric=\"AUROC_test_diff\", yaxixlabel=\"AUROC - AUROC_subsampled\")\n",
    "plt.savefig(\"fig/reasoning_results_diff_full_subsampled_test.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "aa1473f2436c40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(all_datasets_assessors, sort=True, set_axis_bounds=False, metric=\"AUROC_val_diff\", yaxixlabel=\"AUROC - AUROC_subsampled\")\n",
    "plt.savefig(\"fig/reasoning_results_diff_full_subsampled_val.pdf\", bbox_inches=\"tight\")"
   ],
   "id": "68120b168f42d144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3cff0896e08f9790"
   },
   "cell_type": "markdown",
   "source": [
    "Adding training samples is not beneficial for the random splits, but it is (in general, except for ngrams) for the ood split.\n",
    "\n",
    "The fact that ngrams_1 works much better with a larger number of samples for some LLMs in the OOD split is not that surprising: it is likely due to the fact that some words in the OOD set that were not seen for the subsampled train dataset were instead seen for the full dataset. Other large variations happen with word2vec, likely for the same reason."
   ],
   "id": "3cff0896e08f9790"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How many OpenaAI embeddings are needed?\n",
    "\n",
    "I will do this with the OpenAI ones, as I am not sure it makes much sense for the other embeddings (the new OpenAI embeddings use a special technique that makes a subset of them applicable).\n",
    "\n",
    "Notice that, for the KindsOfReasoning dataset, this has been run with the full dataset (i.e., no subsampling was applied)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "a1d990737d01ac19"
   },
   "id": "a1d990737d01ac19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the embedding sizes to try:\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "a244212b7aef0dbf"
   },
   "id": "a244212b7aef0dbf"
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072]"
   ],
   "metadata": {
    "id": "7a9f5eb87ae60077"
   },
   "id": "7a9f5eb87ae60077",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the code"
   ],
   "metadata": {
    "collapsed": false,
    "id": "fba573e40339944d"
   },
   "id": "fba573e40339944d"
  },
  {
   "cell_type": "code",
   "source": [
    "def _check_skip(res_df, pred_method_name, feature_name, split, llm, embedding_size):\n",
    "    if len(res_df)> 0 and len(res_df[(res_df[\"predictive_method\"] == pred_method_name) & (res_df[\"features\"] == feature_name) & (res_df[\"llm\"] == llm) & (res_df[\"split\"] == split) & (res_df[\"embedding_size\"] == embedding_size)]) > 0:\n",
    "        print(f\"Skipping {split}, {feature_name}, {pred_method_name}, {embedding_size} for {llm} because it is already in the dataframe\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Doing {split}, {feature_name}, {pred_method_name}, {embedding_size} for {llm}\")\n",
    "        return False\n",
    "\n",
    "def _concat_and_save(res_df, pred_method_name, feature_name, split, llm, embedding_size,\n",
    "                     BrierScore_val, Calibration_val, Refinement_val, roc_auc_val,\n",
    "                     BrierScore_test, Calibration_test, Refinement_test, roc_auc_test,\n",
    "                     trained_method, filename):\n",
    "    res_df = pd.concat([res_df, pd.DataFrame(\n",
    "    {\"predictive_method\": pred_method_name, \"features\": feature_name, \"split\": split,\n",
    "     \"llm\": llm, \"embedding_size\": embedding_size,\n",
    "    \"BrierScore_val\": BrierScore_val, \"Calibration_val\": Calibration_val, \"Refinement_val\": Refinement_val, \"AUROC_val\": roc_auc_val,\n",
    "     \"BrierScore_test\": BrierScore_test, \"Calibration_test\": Calibration_test, \"Refinement_test\": Refinement_test, \"AUROC_test\": roc_auc_test, \"trained_classifier\": trained_method}, index=[0])])\n",
    "\n",
    "    save_dataframe(filename, res_df)\n",
    "    return res_df\n",
    "\n",
    "def evaluate_and_update(res_df, train_df, validation_df, test_df, features, predictive_method,\n",
    "                        pred_method_name, feature_name, split, llm, embedding_size,\n",
    "                        filename,\n",
    "                        **kwargs):\n",
    "    if not _check_skip(res_df, pred_method_name, feature_name, split, llm, embedding_size):\n",
    "        BrierScore_test, Calibration_test, Refinement_test, roc_auc_test, trained_method = evaluate_predictive_method(train_df, test_df,\n",
    "                                                                                                  features,\n",
    "                                                                                                  f\"Success_{llm}\",\n",
    "                                                                                                  predictive_method=predictive_method,\n",
    "                                                                                                  return_trained_method=True,\n",
    "                                                                                                  **kwargs)\n",
    "        BrierScore_val, Calibration_val, Refinement_val, roc_auc_val = evaluate_predictive_method(train_df, validation_df,\n",
    "                                                                                                  features,\n",
    "                                                                                                  f\"Success_{llm}\",\n",
    "                                                                                                  predictive_method=predictive_method,\n",
    "                                                                                                  trained_method=trained_method,\n",
    "                                                                                                  **kwargs)\n",
    "        res_df = _concat_and_save(res_df, pred_method_name, feature_name, split, llm, embedding_size,\n",
    "                                  BrierScore_val, Calibration_val, Refinement_val, roc_auc_val,\n",
    "                                  BrierScore_test, Calibration_test, Refinement_test, roc_auc_test,\n",
    "                                  trained_method, filename)\n",
    "    return res_df"
   ],
   "metadata": {
    "id": "56b06d44ed8be630"
   },
   "id": "56b06d44ed8be630",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_best_predictive_method_per_feature(increasing_n_embeddings_assessors, sort=False, col_order=None, metric=\"AUROC_test\", font_scale=1, legend_text_size=None, height=5, aspect=1, legend_loc=(1.03, 0.6), panel_space=None):\n",
    "    # for each set of features, extract the best predictive method\n",
    "    best_predictive_method_per_feature = increasing_n_embeddings_assessors.groupby([\"llm\", \"features\", \"embedding_size\", \"split\"]).apply(lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True)\n",
    "    # sort using the order of the models\n",
    "    if sort:\n",
    "        best_predictive_method_per_feature = best_predictive_method_per_feature.sort_values(by=[\"llm\"], key=lambda x: x.apply(lambda y: sort_models_order.index(y)))\n",
    "    else:\n",
    "        best_predictive_method_per_feature = best_predictive_method_per_feature.sort_values(by=[\"llm\"])\n",
    "    best_predictive_method_per_feature[\"split\"] = best_predictive_method_per_feature[\"split\"].astype(str).replace(\"False\", \"In distribution\").replace(\"OOD_1\", \"OOD 1\").replace(\"OOD_2\", \"OOD 2\").replace(\"OOD_3\", \"OOD 3\").replace(\"OOD_4\", \"OOD 4\")\n",
    "\n",
    "    # plot using seaborn\n",
    "    # notice I want to stratify results across language model\n",
    "    # and then put llms on the x\n",
    "    # and then put the predictive method on the hue\n",
    "    # Adjust font scale\n",
    "    sns.set_context(\"notebook\", font_scale=font_scale)\n",
    "\n",
    "    catplot = sns.catplot(data=best_predictive_method_per_feature, x='embedding_size', y=metric, hue='llm', col=\"split\", kind='point', col_order=col_order, height=height, aspect=aspect)\n",
    "\n",
    "    catplot.fig.subplots_adjust(wspace=panel_space)\n",
    "\n",
    "    # title\n",
    "    # plt.title(\"AUROC for different embedding sizes and language models\\nby combining all datasets\")\n",
    "    # set ylim between 0.5 and 1\n",
    "    for ax in plt.gcf().axes:\n",
    "        ax.set_ylim(0.5, 1)\n",
    "    # make lines thinner\n",
    "    for ax in plt.gcf().axes:\n",
    "        for line in ax.lines:\n",
    "            line.set_linewidth(1)\n",
    "    # make points smaller\n",
    "    for ax in plt.gcf().axes:\n",
    "        for line in ax.lines:\n",
    "            line.set_markersize(2)\n",
    "    # rotate x labels\n",
    "    for ax in plt.gcf().axes:\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "    # Adjust legend font size\n",
    "    catplot._legend.set_title(\"Features\")\n",
    "    if legend_text_size is not None:\n",
    "        plt.setp(catplot._legend.get_texts(), fontsize=f'{legend_text_size}')  # for legend text\n",
    "        plt.setp(catplot._legend.get_title(), fontsize=f'{legend_text_size+1}')  # for legend title\n",
    "\n",
    "    # Move legend to an appropriate position\n",
    "    catplot._legend.set_bbox_to_anchor(legend_loc)\n",
    "    catplot._legend.set_frame_on(False)  # Optionally, remove the legend frame for better appearance\n",
    " \n",
    "       "
   ],
   "metadata": {
    "id": "d486e92d8eba3625"
   },
   "id": "d486e92d8eba3625",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KindsOfReasoning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4454036dd6144bf2"
   },
   "id": "4454036dd6144bf2"
  },
  {
   "cell_type": "code",
   "source": [
    "llms = [\n",
    "    'text-ada-001',\n",
    "    'text-babbage-001',\n",
    "    'text-curie-001',\n",
    "    'text-davinci-001',\n",
    "    'text-davinci-002',\n",
    "    'text-davinci-003',\n",
    "    'gpt-3.5-turbo-0301',\n",
    "    'gpt-3.5-turbo-0613',\n",
    "    'gpt-3.5-turbo-1106',\n",
    "    'gpt-3.5-turbo-0125',\n",
    "    'gpt-4-0314',\n",
    "    'gpt-4-0613',\n",
    "    'gpt-4-1106-preview',\n",
    "    'gpt-4-0125-preview',\n",
    "]"
   ],
   "metadata": {
    "id": "30bded7333600459"
   },
   "id": "30bded7333600459",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "filename = \"results/specific_assessor_reasoning_increasing_n_emb.pkl\"",
   "metadata": {
    "id": "99e20fbd47bdaf27"
   },
   "id": "99e20fbd47bdaf27",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "increasing_n_embeddings_assessors = load_with_conditions(filename, overwrite_res)"
   ],
   "metadata": {
    "id": "bc7f8d381b54134"
   },
   "id": "bc7f8d381b54134",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for embedding_size in embedding_sizes:\n",
    "            train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(lambda x: x[:embedding_size])\n",
    "            validation_df[\"openai_embeddings_subset\"] = validation_df[\"openai_embeddings_large\"].apply(lambda x: x[:embedding_size])\n",
    "            test_df[\"openai_embeddings_subset\"] = test_df[\"openai_embeddings_large\"].apply(lambda x: x[:embedding_size])\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                increasing_n_embeddings_assessors = evaluate_and_update(increasing_n_embeddings_assessors, train_df, validation_df, test_df, [\"openai_embeddings_subset\"], predictive_method, pred_method_name, \"openai\", split, llm, embedding_size, filename, **kwargs)"
   ],
   "metadata": {
    "id": "69937bc616e1a10b"
   },
   "id": "69937bc616e1a10b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plots"
   ],
   "metadata": {
    "collapsed": false,
    "id": "501d069373b8d641"
   },
   "id": "501d069373b8d641"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filename = \"results/specific_assessor_reasoning_increasing_n_emb.pkl\"",
   "id": "36c77d08828c6a2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "increasing_n_embeddings_assessors = load_with_conditions(filename)"
   ],
   "metadata": {
    "id": "e3f5edd0c23e4ef6",
    "outputId": "68dfe4f7-3f84-4a0c-933b-56863dcb0ef7"
   },
   "id": "e3f5edd0c23e4ef6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(increasing_n_embeddings_assessors, sort=True, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/reasoning_results_increasing_n_emb_test.pdf\")"
   ],
   "metadata": {
    "id": "e0bdc5a38b8cdd60",
    "outputId": "eb758a9b-19a2-4ff1-f2a5-e7851c390554"
   },
   "id": "e0bdc5a38b8cdd60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(increasing_n_embeddings_assessors, sort=True, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], metric=\"AUROC_val\", legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/reasoning_results_increasing_n_emb_val.pdf\")"
   ],
   "id": "a9e298b9889f3b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HELM-Lite"
   ],
   "metadata": {
    "collapsed": false,
    "id": "60de4e58c948d24a"
   },
   "id": "60de4e58c948d24a"
  },
  {
   "cell_type": "code",
   "source": [
    "llms_dict = {'01-ai/yi-34b': '01-ai/yi-34b',\n",
    "             '01-ai/yi-6b': '01-ai/yi-6b',\n",
    "             'AlephAlpha/luminous-base': 'AlephAlpha/luminous-base',\n",
    "             'AlephAlpha/luminous-extended': 'AlephAlpha/luminous-extended',\n",
    "             'AlephAlpha/luminous-supreme': 'AlephAlpha/luminous-supreme',\n",
    "             'ai21/j2-grande': 'ai21/j2-grande',\n",
    "             'ai21/j2-jumbo': 'ai21/j2-jumbo',\n",
    "             'anthropic/claude-2.0': 'anthropic/claude-2.0',\n",
    "             'anthropic/claude-2.1': 'anthropic/claude-2.1',\n",
    "             'anthropic/claude-instant-1.2': 'anthropic/claude-instant-1.2',\n",
    "             'anthropic/claude-v1.3': 'anthropic/claude-v1.3',\n",
    "             'cohere/command': 'cohere/command',\n",
    "             'cohere/command-light': 'cohere/command-light',\n",
    "             'google/text-bison@001': 'google/text-bison@001',\n",
    "             'google/text-unicorn@001': 'google/text-unicorn@001',\n",
    "             'meta/llama-2-13b': 'meta/llama-2-13b',\n",
    "             'meta/llama-2-70b': 'meta/llama-2-70b',\n",
    "             'meta/llama-2-7b': 'meta/llama-2-7b',\n",
    "             'meta/llama-65b': 'meta/llama-65b',\n",
    "             'mistralai/mistral-7b-v0.1': 'mistralai/mistral-7b-v0.1',\n",
    "             'mistralai/mixtral-8x7b-32kseqlen': 'mistralai/mixtral-8x7b-32kseqlen',\n",
    "             'gpt-3.5-turbo-0613': 'openai/gpt-3.5-turbo-0613',\n",
    "             'gpt-4-0613': 'openai/gpt-4-0613',\n",
    "             'gpt-4-1106-preview': 'openai/gpt-4-1106-preview',\n",
    "             'text-davinci-002': 'openai/text-davinci-002',\n",
    "             'text-davinci-003': 'openai/text-davinci-003',\n",
    "             'tiiuae/falcon-40b': 'tiiuae/falcon-40b',\n",
    "             'tiiuae/falcon-7b': 'tiiuae/falcon-7b',\n",
    "             'writer/palmyra-x-v2': 'writer/palmyra-x-v2',\n",
    "             'writer/palmyra-x-v3': 'writer/palmyra-x-v3'}\n",
    "llms = list(llms_dict.keys())"
   ],
   "metadata": {
    "id": "6572d15776d3f17"
   },
   "id": "6572d15776d3f17",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "filename = \"results/specific_assessors_helm_increasing_n_emb.pkl\"",
   "metadata": {
    "id": "5e40cc2371ae8bbe"
   },
   "id": "5e40cc2371ae8bbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "increasing_n_embeddings_assessors = load_with_conditions(filename, overwrite_res)"
   ],
   "metadata": {
    "id": "d0f19f7bf2d4d00",
    "outputId": "8c8dcb81-329f-4872-e72c-5098fce352c7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d0f19f7bf2d4d00",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings/\")\n",
    "\n",
    "    for llm in llms:\n",
    "        if len(train_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the train df\")\n",
    "            continue\n",
    "\n",
    "        if len(validation_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the validation df\")\n",
    "            continue\n",
    "\n",
    "        if len(test_df[f\"Success_{llm}\"].unique()) < 2:\n",
    "            print(f\"Skipping {llm} because there is only one value in the 'Success' column for the test df\")\n",
    "            continue\n",
    "\n",
    "        for embedding_size in embedding_sizes:\n",
    "            train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(lambda x: x[:embedding_size])\n",
    "            validation_df[\"openai_embeddings_subset\"] = validation_df[\"openai_embeddings_large\"].apply(lambda x: x[:embedding_size])\n",
    "            test_df[\"openai_embeddings_subset\"] = test_df[\"openai_embeddings_large\"].apply(lambda x: x[:embedding_size])\n",
    "\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "\n",
    "                increasing_n_embeddings_assessors = evaluate_and_update(increasing_n_embeddings_assessors, train_df, validation_df, test_df, [\"openai_embeddings_subset\"], predictive_method, pred_method_name, \"openai\", split, llm, embedding_size, filename, **kwargs)"
   ],
   "metadata": {
    "id": "3adba956ab5d6951",
    "outputId": "a8dd58a1-2b2e-4e9e-e43d-b18bffb323b2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "3adba956ab5d6951",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plots\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b38dad003f684262"
   },
   "id": "b38dad003f684262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filename = \"results/specifc_assessors_helm_increasing_n_emb.pkl\"",
   "id": "bc2f8b863552e42a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "increasing_n_embeddings_assessors = load_with_conditions(filename)"
   ],
   "metadata": {
    "id": "d3e9315175d5c54",
    "outputId": "b2791e16-c0ea-4230-f0bf-bf955b5009b3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d3e9315175d5c54",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(increasing_n_embeddings_assessors, sort=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\"], legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/helm_results_increasing_n_emb_test.pdf\")"
   ],
   "metadata": {
    "id": "80688e4d601015ec",
    "outputId": "b1d915dc-25d7-4aa3-d738-0376be4d84cc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    }
   },
   "id": "80688e4d601015ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_per_feature(increasing_n_embeddings_assessors, sort=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\"], legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/helm_results_increasing_n_emb_val.pdf\")"
   ],
   "id": "70b2e2866a8ef4f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
