{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook contain experiments with the generic assessor varying the number of reference instances, to understand the impact of that on the performance of the classifiers.\n",
    "\n",
    "Note that running this notebook may take long and require a substantial amount of RAM."
   ],
   "id": "324c900b4bc78d61"
  },
  {
   "cell_type": "code",
   "source": [
    "!python --version"
   ],
   "metadata": {
    "id": "d87f5c27dafdc9e2",
    "outputId": "aa912022-0743-4676-91a1-63e6a4814115",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d87f5c27dafdc9e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "outputId": "3cb02f3e-8fb6-4c13-e0df-13ad6ca47774",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from src.results_loaders import sort_models_order, load_helm_lite, load_reasoning\n",
    "from src.utils import load_with_conditions, save_dataframe\n",
    "from src.classification_utils import predictive_method_list\n",
    "from src.reference_benchmark import SampleSelector, AssessorFromReference\n",
    "\n",
    "# enable reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Experiment design\n",
    "Take a fixed method and test the various predictive methods by increasing the number of reference points."
   ],
   "id": "ef433d9341b506c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n_embeddings_truncate = 1024",
   "id": "b2becef4993514fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _check_skip(res_df, feature_name, split, selector_name, n_ref, assessor_name, pred_method_name):\n",
    "    \"\"\"pred_method_name is the name of the base classifier, while assessor_name is the name of the method that builds on the base classifier using the reference dataset\n",
    "    \n",
    "    This checks if the experiment has already been done and should be skipped (notice that it does not check for each llm indepenendently as they are all done together\"\"\"\n",
    "    if len(res_df) > 0 and len(res_df[(\n",
    "                                              res_df[\"features\"] == feature_name) & (res_df[\"split\"] == split) &\n",
    "                                      (res_df[\"selector\"] == selector_name) & (\n",
    "                                              res_df[\"assessor\"] == assessor_name) & (\n",
    "                                              res_df[\"predictive_method\"] == pred_method_name) & (\n",
    "                                              res_df[\"n_ref\"] == n_ref)]) > 0:\n",
    "        print(f\"Skipping {feature_name}, {split},  {selector_name}, {assessor_name}, {pred_method_name}, {n_ref}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Doing {feature_name}, {split},  {selector_name}, {assessor_name}, {pred_method_name}, {n_ref}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def _concat_and_save(res_df, prediction_evaluations_validation, prediction_evaluations_test, feature_name, split, selector_name, n_ref, assessor_name,\n",
    "                     pred_method_name, filename):\n",
    "\n",
    "\n",
    "    for prediction_evaluations in [prediction_evaluations_validation, prediction_evaluations_test]:\n",
    "        # prediction_evaluations is a list of dictionaries: \n",
    "        # {\n",
    "        #     \"llm\": llm,\n",
    "        #     \"BrierScore\": BrierScore,\n",
    "        #     \"Calibration\": Calibration,\n",
    "        #     \"Refinement\": Refinement,\n",
    "        #     \"AUROC\": roc_auc,\n",
    "        #     \"Accuracy\": accuracy,\n",
    "        #     \"Predictions\": y_pred,\n",
    "        #     \"subset\": subset\n",
    "        # }\n",
    "        # transform into dataframe and add all the other fields\n",
    "        new_df = pd.DataFrame(prediction_evaluations)\n",
    "        new_df[\"features\"] = feature_name\n",
    "        new_df[\"split\"] = split\n",
    "        new_df[\"selector\"] = selector_name\n",
    "        new_df[\"assessor\"] = assessor_name\n",
    "        new_df[\"n_ref\"] = n_ref\n",
    "        new_df[\"predictive_method\"] = pred_method_name\n",
    "    \n",
    "        # now concatenate to the previous one: \n",
    "        res_df = pd.concat([res_df, new_df])    # prediction_evaluations is a list of dictionaries: \n",
    "    \n",
    "    # save the dataframe\n",
    "    save_dataframe(filename, res_df)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def evaluate_and_update(res_df, feature_name, split, selector_name, n_ref, assessor_name_original, assessor_name_results,\n",
    "                        pred_method_name,\n",
    "                        assessor, predictive_method,\n",
    "                        filename, **kwargs):\n",
    "    if not _check_skip(res_df, feature_name, split, selector_name, n_ref, assessor_name_results, pred_method_name):\n",
    "        \n",
    "        results_per_llm_dict_val, results_per_llm_dict_test = assessor.predict(assessor_name_original, classifier=predictive_method,\n",
    "                                                **kwargs)\n",
    "        prediction_evaluations_validation = assessor.evaluate_predictions(results_per_llm_dict_val, subset=\"validation\")\n",
    "        prediction_evaluations_test = assessor.evaluate_predictions(results_per_llm_dict_test, subset=\"test\")\n",
    "\n",
    "        res_df = _concat_and_save(res_df, prediction_evaluations_validation, prediction_evaluations_test, feature_name, split, selector_name, n_ref,\n",
    "                                  assessor_name_results, pred_method_name, filename)\n",
    "\n",
    "    return res_df\n",
    "\n"
   ],
   "id": "97a86ba65a809690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_selectors(split_name, reference_datasets_dict, reference_datasets_dict_name, selector_methods, n_reference_list,\n",
    "                  train_df, train_llms, n_embeddings_truncate, irt_file_prefix):\n",
    "    # notice test_df is not really used here.\n",
    "    \n",
    "    if split_name not in reference_datasets_dict:\n",
    "        reference_datasets_dict[split_name] = {}\n",
    "\n",
    "    print(\"split name: \", split_name)\n",
    "\n",
    "    # truncate the embeddings\n",
    "    train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "\n",
    "    # now I need to obtain the reference df with the various methods\n",
    "    # define the selector\n",
    "    selector = SampleSelector(train_df, \"openai_embeddings_subset\", train_llms)\n",
    "\n",
    "    # try all possible selection methods\n",
    "    for selector_name in selector_methods:\n",
    "        if selector_name not in reference_datasets_dict[split_name]:\n",
    "            reference_datasets_dict[split_name][selector_name] = {}\n",
    "        for n_reference in n_reference_list:\n",
    "            if n_reference in reference_datasets_dict[split_name][selector_name]:\n",
    "                print(f\"{selector_name} with n_reference {n_reference} already computed\")\n",
    "                selected_df_indeces = reference_datasets_dict[split_name][selector_name][str(n_reference)]\n",
    "            else:\n",
    "                print(f\"Trying {selector_name} with n_reference {n_reference}\")\n",
    "                if \"IRT\" in selector_name:\n",
    "                    selected_df = selector.select(selector_name, n_selected=n_reference,\n",
    "                                                  irt_path=f'data_irt/{irt_file_prefix}_irtmodel/')\n",
    "                else:\n",
    "                    selected_df = selector.select(selector_name, n_selected=n_reference)\n",
    "                if selected_df is None:\n",
    "                    print(f\"Skipping {selector_name} as it did not return any samples\")\n",
    "                    continue\n",
    "                else:\n",
    "                    selected_df_indeces = list(selected_df.index)\n",
    "    \n",
    "                reference_datasets_dict[split_name][selector_name][str(n_reference)] = selected_df_indeces\n",
    "        \n",
    "            print(len(selected_df_indeces))\n",
    "\n",
    "            # save the dict at each iteration\n",
    "            with open(reference_datasets_dict_name, \"w\") as f:\n",
    "                json.dump(reference_datasets_dict, f)\n",
    "    \n",
    "    return reference_datasets_dict\n",
    "    "
   ],
   "id": "e26224f196a66493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_assessor(train_df, validation_df, test_df, train_llms, validation_llms, test_llms, reference_datasets_dict, results_df, results_filename,\n",
    "                 split_name, selector_methods, n_reference_list, assessor_methods_list, predictive_method_list,\n",
    "                 n_embeddings_truncate=1000):\n",
    "    # truncate the embeddings\n",
    "    train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "    validation_df[\"openai_embeddings_subset\"] = validation_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "    test_df[\"openai_embeddings_subset\"] = test_df[\"openai_embeddings_large\"].apply(lambda x: x[:n_embeddings_truncate])\n",
    "    \n",
    "    # convert the split_name to a string if it is the bool False\n",
    "    if isinstance(split_name, bool) and not split_name:\n",
    "        split_name = \"false\"\n",
    "\n",
    "    for selector_name in selector_methods:\n",
    "        if selector_name not in reference_datasets_dict[split_name]:\n",
    "            print(f\"Skipping {selector_name} as it was not computed\")\n",
    "            continue\n",
    "\n",
    "        for n_ref in n_reference_list:\n",
    "            if str(n_ref) not in reference_datasets_dict[split_name][selector_name]:\n",
    "                print(f\"Skipping {selector_name} with n_ref {n_ref} as it was not computed\")\n",
    "                continue\n",
    "            \n",
    "            # now I need to obtain the reference df with the various methods\n",
    "            selected_df_indeces = reference_datasets_dict[split_name][selector_name][str(n_ref)]\n",
    "            selected_df = train_df.loc[selected_df_indeces]\n",
    "    \n",
    "            print(\"selector name: \", selector_name)\n",
    "    \n",
    "            # now define the assessor\n",
    "            assessor = AssessorFromReference(selected_df, train_df, validation_df, test_df, \"openai_embeddings_subset\", train_llms, validation_llms,\n",
    "                                                 test_llms)\n",
    "    \n",
    "            for assessor_name_results, assessor_name_original, assessor_kwargs in assessor_methods_list:\n",
    "                for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "                    results_df = evaluate_and_update(results_df, \"openai\", split_name,\n",
    "                                                     selector_name, n_ref, assessor_name_original, assessor_name_results,\n",
    "                                                     pred_method_name,\n",
    "                                                     assessor, predictive_method, filename=results_filename,\n",
    "                                                     **{**kwargs, **assessor_kwargs})\n",
    "\n",
    "    return results_df"
   ],
   "id": "e030ceebdf9f78e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_best_predictive_method_n_ref_line(all_datasets_assessors, feature=\"openai\", selector=\"clustering_embeddings\",\n",
    "                                           metric=\"AUROC\", use_sort_order=True, hide_legend=True, scale_axis=False,\n",
    "                                           col_order=None, considered_predictive_method=\"xgboost\", subset=\"test\", font_scale=1, legend_text_size=None, height=5, aspect=1, legend_loc=(1.03, 0.6), panel_space=None):\n",
    "    if feature not in all_datasets_assessors[\"features\"].unique():\n",
    "        raise ValueError(f\"Feature {feature} not in the dataset\")\n",
    "\n",
    "    # only consider the test llms:\n",
    "    all_datasets_assessors = all_datasets_assessors[all_datasets_assessors[\"subset\"] == subset]\n",
    "    # only consider the specified predictive method\n",
    "    best_predictive_method_per_feature = all_datasets_assessors[all_datasets_assessors[\"predictive_method\"] == considered_predictive_method]\n",
    "    \n",
    "    # for each set of features, extract the best predictive method -> the way this is done does not make much sense.\n",
    "    # best_predictive_method_per_feature = all_datasets_assessors.groupby([\"llm\", \"features\", \"split\", \"assessor\", \"selector\", \"n_ref\"]).apply(\n",
    "    # lambda x: x[x.AUROC == x.AUROC.max()]).reset_index(drop=True)\n",
    "    # remove duplicates\n",
    "    # best_predictive_method_per_feature = best_predictive_method_per_feature.drop_duplicates(subset=[\"llm\", \"features\", \"split\", \"assessor\", \"selector\", \"n_ref\"])\n",
    "    \n",
    "    # consider only the chosen feature\n",
    "    best_predictive_method_per_feature = best_predictive_method_per_feature[\n",
    "        best_predictive_method_per_feature[\"features\"] == feature]\n",
    "    # consider only the chosen selector\n",
    "    best_predictive_method_per_feature = best_predictive_method_per_feature[\n",
    "        best_predictive_method_per_feature[\"selector\"] == selector]\n",
    "    # sort using the order of the models\n",
    "    best_predictive_method_per_feature = best_predictive_method_per_feature.sort_values(by=[\"llm\"],\n",
    "                                                                                        key=(lambda x: x.apply(lambda\n",
    "                                                                                                                   y: sort_models_order.index(\n",
    "                                                                                            y))) if use_sort_order else None)\n",
    "    # rename the splits:\n",
    "    best_predictive_method_per_feature[\"split\"] = best_predictive_method_per_feature[\"split\"].astype(str).replace(\n",
    "        \"False\", \"In distribution\").replace(\n",
    "        \"false\", \"In distribution\").replace(\"OOD_1\", \"OOD 1\").replace(\"OOD_2\", \"OOD 2\").replace(\"OOD_3\",\n",
    "                                                                                                \"OOD 3\").replace(\n",
    "        \"OOD_4\", \"OOD 4\")\n",
    "\n",
    "    # concatenate llm and assessor:\n",
    "    best_predictive_method_per_feature[\"llm_assessor\"] = best_predictive_method_per_feature[\"llm\"] + \", \" + \\\n",
    "                                                         best_predictive_method_per_feature[\"assessor\"] #+ \" - \" + \\\n",
    "                                                         # best_predictive_method_per_feature[\"predictive_method\"]\n",
    "                                    \n",
    "\n",
    "    # Define your markers\n",
    "    # markers = ['o', 'v', '^', '<', '>', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X']\n",
    "\n",
    "    # Get unique llm values\n",
    "    # unique_llms = best_predictive_method_per_feature['llm'].unique()\n",
    "\n",
    "    # Create a new figure\n",
    "    # fig, axes = plt.subplots(ncols=len(best_predictive_method_per_feature[\"split\"].unique()), nrows=1, figsize=(5*len(best_predictive_method_per_feature[\"split\"].unique()), 5))\n",
    "\n",
    "    # this does not work\n",
    "    # # Loop over unique llms\n",
    "    # for i, llm in enumerate(unique_llms):\n",
    "    #     # Filter dataframe for the current llm\n",
    "    #     df_llm = best_predictive_method_per_feature[best_predictive_method_per_feature['llm'] == llm]\n",
    "    #     sns.pointplot(data=df_llm, x='n_ref', y=metric, hue='assessor', col=\"split\", legend=False, marker=markers[i % len(markers)], ax=axes)\n",
    "\n",
    "    sns.set_context(\"notebook\", font_scale=font_scale)\n",
    "\n",
    "    # Create catplot for the current llm with a specific marker\n",
    "    catplot = sns.catplot(data=best_predictive_method_per_feature, x='n_ref', y=metric, hue='llm_assessor', col=\"split\",\n",
    "                kind=\"point\", legend=not hide_legend,\n",
    "                col_order=col_order, height=height, aspect=aspect)\n",
    "\n",
    "    catplot.fig.subplots_adjust(wspace=panel_space)\n",
    "\n",
    "    # title\n",
    "    # plt.title(\"AUROC for different embedding sizes and language models\\nby combining all datasets\")\n",
    "    # set ylim between 0.5 and 1\n",
    "    if scale_axis:\n",
    "        for ax in plt.gcf().axes:\n",
    "            ax.set_ylim(0.5, 1)\n",
    "    # make lines thinner\n",
    "    for ax in plt.gcf().axes:\n",
    "        for line in ax.lines:\n",
    "            line.set_linewidth(1)\n",
    "    # make points smaller\n",
    "    for ax in plt.gcf().axes:\n",
    "        for line in ax.lines:\n",
    "            line.set_markersize(2)\n",
    "    # rotate x labels\n",
    "    for ax in plt.gcf().axes:\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Adjust legend font size\n",
    "    catplot._legend.set_title(\"Features\")\n",
    "    if legend_text_size is not None:\n",
    "        plt.setp(catplot._legend.get_texts(), fontsize=f'{legend_text_size}')  # for legend text\n",
    "        plt.setp(catplot._legend.get_title(), fontsize=f'{legend_text_size+1}')  # for legend title\n",
    "\n",
    "    # Move legend to an appropriate position\n",
    "    catplot._legend.set_bbox_to_anchor(legend_loc)\n",
    "    catplot._legend.set_frame_on(False)  # Optionally, remove the legend frame for better appearance\n",
    " "
   ],
   "id": "9415716513ac9734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# selector_methods = [\"random\", \"clustering_embeddings\", \"clustering_LLM_success\", \"clustering_IRT_values\", \"factor_analysis_embeddings\", \"factor_analysis_LLM_success_samples\", \"factor_analysis_LLM_success_features\", \"factor_analysis_IRT_values\"]\n",
    "# we will consider here a single selector and predictive method:\n",
    "selector_methods = [\"clustering_embeddings\"]"
   ],
   "id": "7a5ab194b68e2760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "assessor_methods_list = [\n",
    "    (\"baseline_reference_only\", \"reference_only\", {}),\n",
    "    (\"calibrate_general_classifier\", \"calibrate_general_classifier\", {}),\n",
    "    # (\"baseline_all_train_llms\", \"calibrate_general_classifier\", {\"calibration_step\": False}),  # does not depend on the reference set, so can skip this\n",
    "    (\"concatenate_ref_success\", \"concatenate_ref_success\", {}),\n",
    "    (\"concatenate_ref_similarity\", \"concatenate_ref_success\", {\"features\": [\"cosine\"]}),\n",
    "    (\"concatenate_ref_similarity_partial_interaction\", \"concatenate_ref_success\", {\"features\":[\"cosine\"], \"interaction_terms\":\"partial\"}),\n",
    "    # (\"concatenate_ref_similarity_full_interaction\", \"concatenate_ref_success\", {\"features\":[\"cosine\"], \"interaction_terms\":\"full\"}),  this fails due to exchausting RAM\n",
    "]"
   ],
   "id": "7e15849781ad8df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "n_reference_list = [1, 3, 10, 30, 100, 300, 1000]",
   "id": "95ed5a66d0ec6bc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### KindsOfReasoning\n",
   "metadata": {
    "collapsed": false
   },
   "id": "becd50ffbc71dd44"
  },
  {
   "cell_type": "code",
   "source": "results_filename = \"results/generic_assessors_reasoning_increasing_n_ref.pkl\"",
   "metadata": {
    "collapsed": false
   },
   "id": "d413a00c0ca47c46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "n_ref_df = load_with_conditions(results_filename, overwrite_res)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "929c347ab826d9d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import llms_reasoning, train_llms_reasoning, validation_llms_reasoning, test_llms_reasoning\n",
    "llms, train_llms, validation_llms, test_llms = llms_reasoning, train_llms_reasoning, validation_llms_reasoning, test_llms_reasoning"
   ],
   "id": "de99a063fead0493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will first obtain the various reference dfs with the different selectors, so that I do not need to repeat that many times later on.",
   "id": "c3e95bfb2402c2df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "reference_datasets_dict_name = \"results/generic_assessors_dict_reasoning_increasing_n_ref.json\"",
   "id": "ff134e3f88cd9ca3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not os.path.exists(reference_datasets_dict_name) or overwrite_res:\n",
    "    reference_datasets_dict = {}\n",
    "else:\n",
    "    with open(reference_datasets_dict_name, \"r\") as f:\n",
    "        reference_datasets_dict = json.load(f)"
   ],
   "id": "df84efd19ef67a53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next cell runs the selector steps",
   "id": "8b18654a41cfe138"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings\")\n",
    "    \n",
    "    reference_datasets_dict = run_selectors(split, reference_datasets_dict, reference_datasets_dict_name, selector_methods, n_reference_list,\n",
    "                                            train_df,  train_llms, n_embeddings_truncate, irt_file_prefix=f\"reasoning_{split}\")"
   ],
   "id": "790152c2edf21254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now fit the classifiers on all splits, all reference datasets, all predictive frameworks, and all base classifiers.  ",
   "id": "c5846deb0ea8d857"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the reference dictionary\n",
    "with open(reference_datasets_dict_name, \"r\") as f:\n",
    "    reference_datasets_dict = json.load(f)"
   ],
   "id": "e233c1545ee19372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings\")\n",
    "\n",
    "    n_ref_df = run_assessor(train_df, validation_df, test_df, train_llms, validation_llms, test_llms, reference_datasets_dict, n_ref_df, results_filename,\n",
    "                 split, selector_methods, n_reference_list, assessor_methods_list, predictive_method_list, n_embeddings_truncate)"
   ],
   "id": "37ee05d680221a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "#### Plots\n",
   "metadata": {
    "collapsed": false,
    "id": "cf828eaf4a6b87b6"
   },
   "id": "cf828eaf4a6b87b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_filename = \"results/generic_assessors_reasoning_increasing_n_ref.pkl\"\n",
    "all_datasets_assessors = load_with_conditions(results_filename)"
   ],
   "id": "3802e8f90056a7f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dict_assessor = {\n",
    "       'concatenate_ref_similarity': 'Similarity',\n",
    "       'concatenate_ref_similarity_partial_interaction': 'Similarity with interaction',\n",
    "       'concatenate_ref_success': 'Embeddings',\n",
    "}\n",
    "\n",
    "all_datasets_assessors.assessor = all_datasets_assessors.assessor.apply(lambda x: dict_assessor[x] if x in dict_assessor else x)"
   ],
   "id": "208a4d14b5de34fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessors.shape",
   "id": "4bc1952a8b2d172b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessors.columns",
   "id": "4ab116e556b632ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column_name in ['features', 'split', 'selector', 'assessor', 'n_ref', 'predictive_method', 'llm', 'subset']:\n",
    "    print(column_name)\n",
    "    print(all_datasets_assessors[column_name].unique())\n",
    "    print()"
   ],
   "id": "a3b7fa81381ddfaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_datasets_assessors_val = all_datasets_assessors[all_datasets_assessors[\"subset\"] == \"validation\"]\n",
    "all_datasets_assessors_test = all_datasets_assessors[all_datasets_assessors[\"subset\"] == \"test\"]"
   ],
   "id": "632ab196eecc9eee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessors_val.llm.unique()",
   "id": "abb88ef8ec4845f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove the baselines from the plot:",
   "id": "117d9e4cbac4fbd3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessor_plot = all_datasets_assessors[~all_datasets_assessors[\"assessor\"].isin([\"baseline_reference_only\", \"calibrate_general_classifier\"])]",
   "id": "68f10759f3dc2814",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_n_ref_line(all_datasets_assessor_plot, use_sort_order=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], hide_legend=False, subset=\"test\",legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/reasoning_results_increasing_n_ref_test.pdf\")"
   ],
   "id": "21842b7a8ec69a97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_n_ref_line(all_datasets_assessor_plot, use_sort_order=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"], hide_legend=False, subset=\"validation\", legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/reasoning_results_increasing_n_ref_val.pdf\")"
   ],
   "id": "99308aa59cccfce3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HELM-Lite\n",
   "id": "4934b3f01a69867a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "results_filename = \"generic_assessors_helm_increasing_n_ref\"",
   "id": "13c1791c2a917da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "n_ref_df = load_with_conditions(results_filename, overwrite_res)"
   ],
   "id": "846398291a1e8c11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import llms_helm, train_llms_helm, validation_llms_helm, test_llms_helm\n",
    "llms, train_llms, validation_llms, test_llms = llms_helm, train_llms_helm, validation_llms_helm, test_llms_helm"
   ],
   "id": "2415199eff886d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "reference_datasets_dict_name = \"results/generic_assessors_dict_helm_increasing_n_ref.json\"",
   "id": "8c32c2814111ec93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(reference_datasets_dict_name) or overwrite_res:\n",
    "    reference_datasets_dict = {}\n",
    "else:\n",
    "    with open(reference_datasets_dict_name, \"r\") as f:\n",
    "        reference_datasets_dict = json.load(f)"
   ],
   "id": "79e725b889381b77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next instead runs the selector steps",
   "id": "44b2988bbdf3d303"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings\")\n",
    "\n",
    "    reference_datasets_dict = run_selectors(split, reference_datasets_dict, reference_datasets_dict_name, selector_methods, n_reference_list,\n",
    "                                            train_df, train_llms, n_embeddings_truncate, irt_file_prefix=f\"helm_{split}\")"
   ],
   "id": "1892a0a49ca088fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now fit the classifiers on all splits, all reference datasets, all predictive frameworks, and all base classifiers.  ",
   "id": "2d287e8e7b8886ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the reference dictionary\n",
    "with open(reference_datasets_dict_name, \"r\") as f:\n",
    "    reference_datasets_dict = json.load(f)"
   ],
   "id": "2f363a1892ac0e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings\")\n",
    "\n",
    "    n_ref_df = run_assessor(train_df, validation_df, test_df, train_llms, validation_llms, test_llms, reference_datasets_dict, n_ref_df, results_filename,\n",
    "                 split, selector_methods, n_reference_list, assessor_methods_list, predictive_method_list, n_embeddings_truncate)"
   ],
   "id": "db6ad44e0f83e1a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plots\n",
   "id": "b727d63efa75c25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_filename = \"results/generic_assessors_helm_increasing_n_ref.pkl\"\n",
    "all_datasets_assessors = load_with_conditions(results_filename)"
   ],
   "id": "17f4b9bfc53f66a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dict_assessor = {\n",
    "       'concatenate_ref_similarity': 'Similarity',\n",
    "       'concatenate_ref_similarity_partial_interaction': 'Similarity with interaction',\n",
    "       'concatenate_ref_success': 'Embeddings',\n",
    "}\n",
    "\n",
    "all_datasets_assessors.assessor = all_datasets_assessors.assessor.apply(lambda x: dict_assessor[x] if x in dict_assessor else x)"
   ],
   "id": "17af46d48eaefed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessors.shape",
   "id": "f9f5d0fee5a44c16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessors.columns",
   "id": "ce2043726b9aed3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_datasets_assessor_plot = all_datasets_assessors[~all_datasets_assessors[\"assessor\"].isin([\"baseline_reference_only\", \"calibrate_general_classifier\"])]",
   "id": "21ec43d3e936151b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_n_ref_line(all_datasets_assessor_plot, use_sort_order=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\"], hide_legend=False, subset=\"test\", legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5) )\n",
    "plt.savefig(\"fig/helm_results_increasing_n_ref_test.pdf\")"
   ],
   "id": "f6a4c175c0b3f7e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_best_predictive_method_n_ref_line(all_datasets_assessor_plot, use_sort_order=False, col_order=[\"In distribution\", \"OOD 1\", \"OOD 2\", \"OOD 3\"], hide_legend=False, subset=\"validation\", legend_text_size=16, aspect=0.7, font_scale=1.5, panel_space=0.1, legend_loc=(1, 0.5))\n",
    "plt.savefig(\"fig/helm_results_increasing_n_ref_val.pdf\")"
   ],
   "id": "98380054cc67f3fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
