{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "\n",
    "Code for the generic assessor experiments.\n",
    "Note that running this notebook may take long and require a substantial amount of RAM."
   ],
   "id": "aa2688ef35adbb3f"
  },
  {
   "cell_type": "code",
   "source": "!python --version",
   "metadata": {
    "id": "d87f5c27dafdc9e2",
    "outputId": "9af622c1-02dc-4e66-f44c-94147165d453",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "d87f5c27dafdc9e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-08-30T15:37:42.295874Z",
     "start_time": "2024-08-30T15:37:40.264132Z"
    }
   },
   "source": [
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from src.results_loaders import sort_models_order, load_reasoning, load_helm_lite\n",
    "from src.classification_utils import predictive_method_list\n",
    "from src.utils import load_with_conditions, save_dataframe\n",
    "from src.reference_benchmark import SampleSelector, AssessorFromReference\n",
    "\n",
    "# enable reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "ef433d9341b506c6"
   },
   "cell_type": "markdown",
   "source": [
    "##  Experiment design\n",
    "\n",
    "- 2 sets of datasets (reasoning and HELM-Lite) -> 6 splits total; the reasoning one is subsampled to have similar training size as HELM-Lite\n",
    "- 9 selector methods (random, random_best_of, clustering_embeddings, clustering_LLM_success, clustering_IRT_values, factor_analysis_embeddings,  factor_analysis_IRT_values, factor_analysis_LLM_success_samples, factor_analysis_LLM_success_features)\n",
    "- predictive frameworks: reference_only, calibrate_general_classifier, baseline_all_train_llms and concatenate_ref_success (the latter with/without similarity measures as well as full/partial cross products with the similarity measures) -> 7 total\n",
    "- all possible base classifiers (LogReg, XGBoost, ...) -> n classifiers\n",
    "\n",
    "Total is 441 * n experiments.\n",
    "\n",
    "I will also truncate the openAI embeddings to the first 1024, as there does not seem to be any improvement beyond that.\n",
    "\n",
    "Tuning experiments:\n",
    "- how many reference points: to determine this, take the best selector and predictive method, I vary the number of reference points and see how the performance changes (other notebook)\n"
   ],
   "id": "ef433d9341b506c6"
  },
  {
   "metadata": {
    "id": "be152397acbc1ae4"
   },
   "cell_type": "markdown",
   "source": [
    "**Comment on diversity of labels in the reference df**: The reference df shoud have different labels for all test_llms, as otherwise the  reference_only baseline and the recalibrator method would not work. I have put an exception to signal when that is not the case in the source code for that."
   ],
   "id": "be152397acbc1ae4"
  },
  {
   "metadata": {
    "id": "20737fb1badda03f"
   },
   "cell_type": "markdown",
   "source": "**Note**: the IRT values are computed by running an external notebook (`2_run_irt.ipynb`), which must be run before the present one is executed.",
   "id": "20737fb1badda03f"
  },
  {
   "metadata": {
    "id": "b2becef4993514fc"
   },
   "cell_type": "code",
   "source": [
    "n_embeddings_truncate = 1024"
   ],
   "id": "b2becef4993514fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "97a86ba65a809690"
   },
   "cell_type": "code",
   "source": [
    "def _check_skip(res_df, feature_name, split, selector_name, assessor_name, pred_method_name):\n",
    "    \"\"\"pred_method_name is the name of the base classifier, while assessor_name is the name of the method that builds on the base classifier using the reference dataset\n",
    "\n",
    "    This checks if the experiment has already been done and should be skipped (notice that it does not check for each llm indepenendently as they are all done together\"\"\"\n",
    "    if len(res_df) > 0 and len(res_df[(\n",
    "                                              res_df[\"features\"] == feature_name) & (res_df[\"split\"] == split) &\n",
    "                                      (res_df[\"selector\"] == selector_name) & (\n",
    "                                              res_df[\"assessor\"] == assessor_name) & (\n",
    "                                              res_df[\"predictive_method\"] == pred_method_name)]) > 0:\n",
    "        print(f\"Skipping {feature_name}, {split},  {selector_name}, {assessor_name}, {pred_method_name}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Doing {feature_name}, {split},  {selector_name}, {assessor_name}, {pred_method_name}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def _concat_and_save(res_df, prediction_evaluations_validation, prediction_evaluations_test, feature_name, split, selector_name, assessor_name,\n",
    "                     pred_method_name, filename):\n",
    "    for prediction_evaluations in [prediction_evaluations_validation, prediction_evaluations_test]:\n",
    "        # prediction_evaluations is a list of dictionaries:\n",
    "        # {\n",
    "        #     \"llm\": llm,\n",
    "        #     \"BrierScore\": BrierScore,\n",
    "        #     \"Calibration\": Calibration,\n",
    "        #     \"Refinement\": Refinement,\n",
    "        #     \"AUROC\": roc_auc,\n",
    "        #     \"Accuracy\": accuracy,\n",
    "        #     \"Predictions\": y_pred,\n",
    "        #     \"subset\": subset\n",
    "        # }\n",
    "        # transform into dataframe and add all the other fields\n",
    "        new_df = pd.DataFrame(prediction_evaluations)\n",
    "        new_df[\"features\"] = feature_name\n",
    "        new_df[\"split\"] = split\n",
    "        new_df[\"selector\"] = selector_name\n",
    "        new_df[\"assessor\"] = assessor_name\n",
    "        new_df[\"predictive_method\"] = pred_method_name\n",
    "\n",
    "        # now concatenate to the previous one:\n",
    "        res_df = pd.concat([res_df, new_df])\n",
    "\n",
    "    # save the dataframe\n",
    "    save_dataframe(filename, res_df)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def evaluate_and_update(res_df, feature_name, split, selector_name, assessor_name_original, assessor_name_results,\n",
    "                        pred_method_name, assessor, predictive_method,\n",
    "                        filename, **kwargs):\n",
    "    if not _check_skip(res_df, feature_name, split, selector_name, assessor_name_results, pred_method_name):\n",
    "\n",
    "        results_per_llm_dict_val, results_per_llm_dict_test = assessor.predict(assessor_name_original, classifier=predictive_method,\n",
    "                                                **kwargs)\n",
    "        prediction_evaluations_validation = assessor.evaluate_predictions(results_per_llm_dict_val, subset=\"validation\")\n",
    "        prediction_evaluations_test = assessor.evaluate_predictions(results_per_llm_dict_test, subset=\"test\")\n",
    "\n",
    "        res_df = _concat_and_save(res_df, prediction_evaluations_validation, prediction_evaluations_test, feature_name, split, selector_name,\n",
    "                                  assessor_name_results, pred_method_name, filename)\n",
    "\n",
    "    return res_df\n",
    "\n"
   ],
   "id": "97a86ba65a809690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e26224f196a66493"
   },
   "cell_type": "code",
   "source": [
    "def run_selectors(split_name, reference_datasets_dict, reference_datasets_dict_name, selector_methods, n_reference,\n",
    "                  train_df, train_llms, n_embeddings_truncate, irt_file_prefix):\n",
    "\n",
    "    if split_name not in reference_datasets_dict:\n",
    "        reference_datasets_dict[split_name] = {}\n",
    "\n",
    "    print(\"split name: \", split_name)\n",
    "\n",
    "    # truncate the embeddings\n",
    "    train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "\n",
    "    # now I need to obtain the reference df with the various methods\n",
    "    # define the selector\n",
    "    selector = SampleSelector(train_df, \"openai_embeddings_subset\", train_llms)\n",
    "\n",
    "    # try all possible selection methods\n",
    "    for selector_name in selector_methods:\n",
    "        if selector_name in reference_datasets_dict[split_name]:\n",
    "            print(f\"{selector_name} was already computed\")\n",
    "            selected_df_indeces = reference_datasets_dict[split_name][selector_name]\n",
    "        else:\n",
    "            print(f\"Trying selector_name {selector_name}\")\n",
    "            if \"IRT\" in selector_name:\n",
    "                selected_df = selector.select(selector_name, n_selected=n_reference,\n",
    "                                              irt_path=f'data_irt/{irt_file_prefix}_irtmodel/')\n",
    "            else:\n",
    "                selected_df = selector.select(selector_name, n_selected=n_reference)\n",
    "            if selected_df is None:\n",
    "                print(f\"Skipping {selector_name} as it did not return any samples\")\n",
    "                continue\n",
    "            else:\n",
    "                selected_df_indeces = list(selected_df.index)\n",
    "\n",
    "        print(len(selected_df_indeces))\n",
    "        reference_datasets_dict[split_name][selector_name] = selected_df_indeces\n",
    "\n",
    "        # save the dict at each iteration\n",
    "        with open(reference_datasets_dict_name, \"w\") as f:\n",
    "            json.dump(reference_datasets_dict, f)\n",
    "\n",
    "    return reference_datasets_dict\n",
    ""
   ],
   "id": "e26224f196a66493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e030ceebdf9f78e9"
   },
   "cell_type": "code",
   "source": [
    "def run_assessor(train_df, validation_df, test_df, train_llms, validation_llms, test_llms, reference_datasets_dict, results_df, results_filename,\n",
    "                 split_name, selector_methods, assessor_methods_list, predictive_method_list,\n",
    "                 n_embeddings_truncate=1000):\n",
    "    # truncate the embeddings\n",
    "    train_df[\"openai_embeddings_subset\"] = train_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "    validation_df[\"openai_embeddings_subset\"] = validation_df[\"openai_embeddings_large\"].apply(\n",
    "        lambda x: x[:n_embeddings_truncate])\n",
    "    test_df[\"openai_embeddings_subset\"] = test_df[\"openai_embeddings_large\"].apply(lambda x: x[:n_embeddings_truncate])\n",
    "\n",
    "    # convert the split_name to a string if it is the bool False\n",
    "    if isinstance(split_name, bool) and not split_name:\n",
    "        split_name = \"false\"\n",
    "\n",
    "    for selector_name in selector_methods:\n",
    "        if selector_name not in reference_datasets_dict[split_name]:\n",
    "            print(f\"Skipping {selector_name} as it was not computed\")\n",
    "            continue\n",
    "\n",
    "        # extract the reference instances\n",
    "        selected_df_indeces = reference_datasets_dict[split_name][selector_name]\n",
    "        selected_df = train_df.loc[selected_df_indeces]\n",
    "\n",
    "        print(\"selector name: \", selector_name)\n",
    "\n",
    "        # now define the assessor\n",
    "        assessor = AssessorFromReference(selected_df, train_df, validation_df, test_df, \"openai_embeddings_subset\", train_llms,\n",
    "                                             validation_llms, test_llms)\n",
    "\n",
    "        for assessor_name_results, assessor_name_original, assessor_kwargs in assessor_methods_list:\n",
    "            for predictive_method, kwargs, pred_method_name in predictive_method_list:\n",
    "                results_df = evaluate_and_update(results_df, \"openai\", split_name,\n",
    "                                                 selector_name, assessor_name_original, assessor_name_results,\n",
    "                                                 pred_method_name,\n",
    "                                                 assessor, predictive_method, filename=results_filename,\n",
    "                                                 **{**kwargs, **assessor_kwargs})\n",
    "\n",
    "    return results_df"
   ],
   "id": "e030ceebdf9f78e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9526fea3c381b0e8"
   },
   "cell_type": "code",
   "source": [
    "def plot_best_predictive_method_heatmap(all_datasets_assessors, feature=\"openai\", metric=\"AUROC\", use_sort_order=True):\n",
    "\n",
    "    if feature not in all_datasets_assessors[\"features\"].unique():\n",
    "        raise ValueError(f\"Feature {feature} not in the dataset\")\n",
    "\n",
    "    # for each set of features, extract the best predictive method\n",
    "    best_predictive_method_per_feature = all_datasets_assessors.groupby([\"llm\", \"features\", \"split\", \"assessor\", \"selector\"]).apply(\n",
    "    lambda x: x[x.AUROC == x.AUROC.max()]).reset_index(drop=True)\n",
    "    # remove duplicates\n",
    "    best_predictive_method_per_feature = best_predictive_method_per_feature.drop_duplicates(subset=[\"llm\", \"features\", \"split\", \"assessor\", \"selector\"])\n",
    "    # consider only the chosen feature\n",
    "    best_predictive_method_per_feature = best_predictive_method_per_feature[best_predictive_method_per_feature[\"features\"] == feature]\n",
    "    # sort using the order of the models\n",
    "    best_predictive_method_per_feature = best_predictive_method_per_feature.sort_values(by=[\"llm\"],\n",
    "                                                                                        key=(lambda x: x.apply(lambda\n",
    "                                                                                                                  y: sort_models_order.index(\n",
    "                                                                                            y))) if use_sort_order else None)\n",
    "\n",
    "    # better labels for x and y axis of heatmap\n",
    "    best_predictive_method_per_feature[\"selector\"] = best_predictive_method_per_feature[\"selector\"].apply(\n",
    "        lambda x: x.replace(\"_\", \" \").replace(\"clustering\", \"Cluster\").replace(\"factor analysis\", \"FA\").replace(\"samples\", \"\").replace(\"values\", \"\").capitalize())\n",
    "    best_predictive_method_per_feature[\"assessor\"] = best_predictive_method_per_feature[\"assessor\"].apply(\n",
    "        lambda x: x.replace(\"_\", \" \").replace(\"partial \", \"\").capitalize())\n",
    "\n",
    "    # create one heatmap for each split and for each LLM\n",
    "    for split in best_predictive_method_per_feature[\"split\"].unique():\n",
    "        for llm in best_predictive_method_per_feature[\"llm\"].unique():\n",
    "            # print(f\"Split: {split}, LLM: {llm}\")\n",
    "            #filter the dataframe\n",
    "            best_predictive_method_per_feature_split_llm = best_predictive_method_per_feature[\n",
    "                (best_predictive_method_per_feature[\"split\"] == split) & (best_predictive_method_per_feature[\"llm\"] == llm)]\n",
    "            # create the heatmap\n",
    "            plt.figure(figsize=(1*len(best_predictive_method_per_feature_split_llm[\"selector\"].unique()), 1*len(best_predictive_method_per_feature_split_llm[\"assessor\"].unique())))\n",
    "            sns.heatmap(data=best_predictive_method_per_feature_split_llm.pivot(\"assessor\", \"selector\", metric),\n",
    "                        annot=True, fmt=\".2f\", cmap=\"viridis\", vmin=0.5, vmax=1)\n",
    "            plt.title(f\"{metric} for different selectors and assessors\\nfor {llm} and {feature} features\\n{split}\")\n",
    "            plt.show()\n",
    "\n",
    "            # rotate x labels\n",
    "            for ax in plt.gcf().axes:\n",
    "                plt.sca(ax)\n",
    "                plt.xticks(rotation=90)\n"
   ],
   "id": "9526fea3c381b0e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "7a5ab194b68e2760"
   },
   "cell_type": "code",
   "source": [
    "# I removed factor_analysis_LLM_success_features as that fails (singular matrix)\n",
    "selector_methods = [\"random\", \"random_best_of\", \"clustering_embeddings\", \"clustering_LLM_success\", \"clustering_IRT_values\", \"factor_analysis_embeddings\", \"factor_analysis_LLM_success_samples\", \"factor_analysis_IRT_values\"]"
   ],
   "id": "7a5ab194b68e2760",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "7e15849781ad8df"
   },
   "cell_type": "code",
   "source": [
    "assessor_methods_list = [\n",
    "    (\"baseline_reference_only\", \"reference_only\", {}),\n",
    "    (\"calibrate_general_classifier\", \"calibrate_general_classifier\", {}),\n",
    "    (\"baseline_all_train_llms\", \"calibrate_general_classifier\", {\"calibration_step\": False}),\n",
    "    (\"concatenate_ref_success\", \"concatenate_ref_success\", {}),\n",
    "    (\"concatenate_ref_similarity\", \"concatenate_ref_success\", {\"features\": [\"cosine\"]}),\n",
    "    (\"concatenate_ref_similarity_partial_interaction\", \"concatenate_ref_success\", {\"features\":[\"cosine\"], \"interaction_terms\":\"partial\"}),\n",
    "    # (\"concatenate_ref_similarity_full_interaction\", \"concatenate_ref_success\", {\"features\":[\"cosine\"], \"interaction_terms\":\"full\"}),  # requires too much RAM\n",
    "]"
   ],
   "id": "7e15849781ad8df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d1f4cf25d71e002d"
   },
   "cell_type": "markdown",
   "source": [
    "### KindsOfReasoning\n",
    "\n",
    "Here I will keep all GPT4 versions out of the training set of LLMs."
   ],
   "id": "d1f4cf25d71e002d"
  },
  {
   "metadata": {
    "id": "1d89c5e4d53e8560"
   },
   "cell_type": "code",
   "source": "results_filename = \"results/generic_assessors_reasoning.pkl\"",
   "id": "1d89c5e4d53e8560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "acd1c26b47576aa0",
    "outputId": "6c52e4e4-1898-4366-96e0-cf730c861baf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "results_df = load_with_conditions(results_filename, overwrite_res)"
   ],
   "id": "acd1c26b47576aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b6ee25bbf28a2270"
   },
   "cell_type": "code",
   "source": [
    "from src.utils import llms_reasoning, train_llms_reasoning, validation_llms_reasoning, test_llms_reasoning\n",
    "llms, train_llms, validation_llms, test_llms = llms_reasoning, train_llms_reasoning, validation_llms_reasoning, test_llms_reasoning"
   ],
   "id": "b6ee25bbf28a2270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "149c35abc6e2ad55"
   },
   "cell_type": "markdown",
   "source": [
    "I will first obtain the various reference dfs with the different selectors, so that I do not need to repeat that many times later on."
   ],
   "id": "149c35abc6e2ad55"
  },
  {
   "metadata": {
    "id": "395b35d6f5736d4e"
   },
   "cell_type": "code",
   "source": [
    "n_reference = 100\n",
    "reference_datasets_dict_name = \"results/generic_assessors_dict_reasoning.json\""
   ],
   "id": "395b35d6f5736d4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ad999097ca03cebe"
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(reference_datasets_dict_name) or overwrite_res:\n",
    "    reference_datasets_dict = {}\n",
    "else:\n",
    "    with open(reference_datasets_dict_name, \"r\") as f:\n",
    "        reference_datasets_dict = json.load(f)"
   ],
   "id": "ad999097ca03cebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "23efcb5ddbb058f"
   },
   "cell_type": "markdown",
   "source": [
    "The next cell runs the selector steps"
   ],
   "id": "23efcb5ddbb058f"
  },
  {
   "metadata": {
    "id": "cee8b1e66806bdbb",
    "outputId": "58f604ef-f391-4f1f-b544-de5d4a79ab80"
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings\")\n",
    "\n",
    "    reference_datasets_dict = run_selectors(split, reference_datasets_dict, reference_datasets_dict_name, selector_methods, n_reference,\n",
    "                                            train_df, train_llms, n_embeddings_truncate, irt_file_prefix=f\"reasoning_{split}\")"
   ],
   "id": "cee8b1e66806bdbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d1cd97234ddeb3b6"
   },
   "cell_type": "markdown",
   "source": [
    "Now fit the classifiers on all splits, all reference datasets, all predictive frameworks, and all base classifiers.  "
   ],
   "id": "d1cd97234ddeb3b6"
  },
  {
   "metadata": {
    "id": "fc5bae0dc18314fe"
   },
   "cell_type": "code",
   "source": [
    "# load the reference dictionary\n",
    "with open(reference_datasets_dict_name, \"r\") as f:\n",
    "    reference_datasets_dict = json.load(f)"
   ],
   "id": "fc5bae0dc18314fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3f705eaeb044a628",
    "outputId": "2287c0f7-cdc4-4a6d-948f-86661862ba48",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]:\n",
    "    train_df, validation_df, test_df = load_reasoning(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/kindsofreasoning_embeddings\")\n",
    "\n",
    "    print(\"split name: \", split)\n",
    "\n",
    "    results_df = run_assessor(train_df,validation_df, test_df, train_llms, validation_llms, test_llms, reference_datasets_dict, results_df, results_filename,\n",
    "                 split, selector_methods, assessor_methods_list, predictive_method_list, n_embeddings_truncate)"
   ],
   "id": "3f705eaeb044a628",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "99f9670455ffe328"
   },
   "cell_type": "markdown",
   "source": "#### Plots\n",
   "id": "99f9670455ffe328"
  },
  {
   "metadata": {
    "id": "6a8f4dc009d47b01",
    "outputId": "41f01981-449e-4710-a37a-831a8f3ccdba",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "results_filename = \"results/generic_assessors_reasoning.pkl\"\n",
    "all_datasets_assessors = load_with_conditions(results_filename)"
   ],
   "id": "6a8f4dc009d47b01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ae2709f3b3af1aa1",
    "outputId": "7188a614-bc99-4405-ae4c-b0d8fa36d4cc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors.shape"
   ],
   "id": "ae2709f3b3af1aa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ec839ab043ba61c0",
    "outputId": "739b35f9-41a4-45cf-b448-b0b2384702e4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors.columns"
   ],
   "id": "ec839ab043ba61c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6510440e2cc73a56",
    "outputId": "d61685e9-2278-4cf1-8fbf-97a056581164",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors[\"features\"].value_counts()"
   ],
   "id": "6510440e2cc73a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f56c66aa13420236"
   },
   "cell_type": "markdown",
   "source": [
    "- Possible metrics are 'BrierScore', 'Calibration', 'Refinement', 'AUROC', 'Accuracy'\n",
    "- the independent features are instead 'llm', 'features', 'split', 'assessor', 'selector'\n",
    "- \"features\" is only \"openai\", and split has few values."
   ],
   "id": "f56c66aa13420236"
  },
  {
   "metadata": {
    "id": "96628c64f36e82e6"
   },
   "cell_type": "markdown",
   "source": [
    "The following shows the performance of the best classifier for each assessor and selector on validation data and LLMs."
   ],
   "id": "96628c64f36e82e6"
  },
  {
   "metadata": {
    "id": "cc1f2e96db53af0d",
    "outputId": "8d5ad59b-6978-43e6-d22e-e3a5c4ef794a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    }
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors_validation = all_datasets_assessors[all_datasets_assessors[\"subset\"] == \"validation\"]\n",
    "\n",
    "plot_best_predictive_method_heatmap(all_datasets_assessors_validation)"
   ],
   "id": "cc1f2e96db53af0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4934b3f01a69867a"
   },
   "cell_type": "markdown",
   "source": [
    "### HELM-Lite"
   ],
   "id": "4934b3f01a69867a"
  },
  {
   "metadata": {
    "id": "13c1791c2a917da"
   },
   "cell_type": "code",
   "source": "results_filename = \"results/generic_assessors_helm.pkl\"",
   "id": "13c1791c2a917da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "846398291a1e8c11"
   },
   "cell_type": "code",
   "source": [
    "overwrite_res = False\n",
    "\n",
    "results_df = load_with_conditions(results_filename, overwrite_res)"
   ],
   "id": "846398291a1e8c11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c968bba85e1d7c52"
   },
   "cell_type": "code",
   "source": [
    "from src.utils import llms_helm, train_llms_helm, validation_llms_helm, test_llms_helm\n",
    "llms, train_llms, validation_llms, test_llms = llms_helm, train_llms_helm, validation_llms_helm, test_llms_helm"
   ],
   "id": "c968bba85e1d7c52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8c32c2814111ec93"
   },
   "cell_type": "code",
   "source": [
    "n_reference = 100\n",
    "reference_datasets_dict_name = \"results/generic_assessors_dict_helm.json\""
   ],
   "id": "8c32c2814111ec93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "79e725b889381b77"
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(reference_datasets_dict_name) or overwrite_res:\n",
    "    reference_datasets_dict = {}\n",
    "else:\n",
    "    with open(reference_datasets_dict_name, \"r\") as f:\n",
    "        reference_datasets_dict = json.load(f)"
   ],
   "id": "79e725b889381b77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "44b2988bbdf3d303"
   },
   "cell_type": "markdown",
   "source": [
    "The next instead runs the selector steps"
   ],
   "id": "44b2988bbdf3d303"
  },
  {
   "metadata": {
    "id": "1892a0a49ca088fc",
    "outputId": "979656f5-c3f5-4991-c744-8d2305eb2c5d"
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings\")\n",
    "\n",
    "    reference_datasets_dict = run_selectors(split, reference_datasets_dict, reference_datasets_dict_name, selector_methods, n_reference,\n",
    "                                            train_df, train_llms, n_embeddings_truncate, irt_file_prefix=f\"helm_{split}\")"
   ],
   "id": "1892a0a49ca088fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2d287e8e7b8886ac"
   },
   "cell_type": "markdown",
   "source": [
    "Now fit the classifiers on all splits, all reference datasets, all predictive frameworks, and all base classifiers.  "
   ],
   "id": "2d287e8e7b8886ac"
  },
  {
   "metadata": {
    "id": "2f363a1892ac0e98"
   },
   "cell_type": "code",
   "source": [
    "# load the reference dictionary\n",
    "with open(reference_datasets_dict_name, \"r\") as f:\n",
    "    reference_datasets_dict = json.load(f)"
   ],
   "id": "2f363a1892ac0e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "db6ad44e0f83e1a5",
    "outputId": "929087f1-e203-4c8a-a0dc-111d6bf4b3b0"
   },
   "cell_type": "code",
   "source": [
    "for split in [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]:\n",
    "    train_df, validation_df, test_df = load_helm_lite(llms, [\"openai_embeddings\"], ood_split=split, base_path=\"../results/helm_lite_v1.0.0_embeddings\")\n",
    "\n",
    "    print(\"split name: \", split)\n",
    "\n",
    "    results_df = run_assessor(train_df, validation_df, test_df, train_llms, validation_llms, test_llms, reference_datasets_dict, results_df, results_filename,\n",
    "                 split, selector_methods, assessor_methods_list, predictive_method_list, n_embeddings_truncate)"
   ],
   "id": "db6ad44e0f83e1a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b727d63efa75c25"
   },
   "cell_type": "markdown",
   "source": [
    "#### Plots\n"
   ],
   "id": "b727d63efa75c25"
  },
  {
   "metadata": {
    "id": "bc119a4bfdeec5a1",
    "outputId": "d65643c9-0862-41ec-f9ad-e0b837e37951"
   },
   "cell_type": "code",
   "source": [
    "results_filename = \"results/generic_assessors_helm.pkl\"\n",
    "all_datasets_assessors = load_with_conditions(results_filename)"
   ],
   "id": "bc119a4bfdeec5a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "98638c1a0872833e",
    "outputId": "458c1d11-585a-43df-985f-5d97b8c1675b"
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors.shape"
   ],
   "id": "98638c1a0872833e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "51d0080600e716e5"
   },
   "cell_type": "markdown",
   "source": [
    "The following shows the performance of the best classifier for each assessor and selector on validation data and LLMs."
   ],
   "id": "51d0080600e716e5"
  },
  {
   "metadata": {
    "id": "69e899f841db96f3",
    "outputId": "0b5b1b9f-8388-4146-b429-309d9997f918"
   },
   "cell_type": "code",
   "source": [
    "all_datasets_assessors_validation = all_datasets_assessors[all_datasets_assessors[\"subset\"] == \"validation\"]\n",
    "\n",
    "plot_best_predictive_method_heatmap(all_datasets_assessors_validation, use_sort_order=False)"
   ],
   "id": "69e899f841db96f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2329f27ee67db01f"
   },
   "cell_type": "markdown",
   "source": [
    "## Select best assessor-selector\n",
    "\n",
    "I will rely on the average win rate of each assessor-selector combination on all others, which has a 1-1 correspondence with the ranking, but it is easier to interpret and to average across scenarios (LLMs, splits, datasets).\n",
    "\n",
    "For each dataset and split, I will select the best (selector, assessor setup, classifier) by considering the average win on validation data and LLMs. I will also do the same by restricting to the reference_only assessor setup, to determine the best classifier and selector. The I will have a best setup for each dataset and split, and a best setup for each dataset and split with the reference_only assessor setup.\n",
    "\n"
   ],
   "id": "2329f27ee67db01f"
  },
  {
   "metadata": {
    "id": "f4c77714e89cd96c"
   },
   "cell_type": "code",
   "source": [
    "from src.utils import validation_llms_reasoning, validation_llms_helm"
   ],
   "id": "f4c77714e89cd96c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "53986c1d3f57a168",
    "outputId": "ca0c18df-ae45-4042-b7f2-9e6b6331352e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "results_reference_reasoning = load_with_conditions(\"results/generic_assessors_reasoning.pkl\")\n",
    "results_reference_helm = load_with_conditions(\"results/generic_assessors_helm.pkl\")"
   ],
   "id": "53986c1d3f57a168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "e27ff661f7f8b4a4",
    "outputId": "7bb9b311-426f-487e-ca9d-82082ee54d12",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "results_reference_reasoning.columns"
   ],
   "id": "e27ff661f7f8b4a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3248c71384620425"
   },
   "cell_type": "code",
   "source": [
    "split_reasoning = [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]\n",
    "split_helm = [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]"
   ],
   "id": "3248c71384620425",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "c39ad6b4a2af22ae"
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_win_rate_single_llm_split_dataset(results, considered_assessors, considered_selectors,\n",
    "                                              considered_classifiers, llm, split, metric=\"AUROC\", features=\"openai\"):\n",
    "    \"\"\"For all combinations of the considered assessor, selector and classifiers, compute whether the assessor-selector-classifier combination is better or worse than any other assessor-selector-classifier combination;\n",
    "    basically compute a binary matrix where 1 means that the assessor-selector-classifier combination is better than the other one, 0 means it is worse. The number of dimensions of the matrix is the number of combinations obtained with considered assessors, selectors and classifiers.\"\"\"\n",
    "\n",
    "    if isinstance(split, bool) and not split:\n",
    "      split = \"false\"\n",
    "\n",
    "    # Filter the DataFrame for the specific LLM, split, and features, and consider only the validation subset\n",
    "    filtered_results = results[\n",
    "        (results['llm'] == llm) & (results['split'] == split) & (results['features'] == features) & (\n",
    "                    results['subset'] == \"validation\")]\n",
    "\n",
    "\n",
    "    # Generate all possible combinations of assessors, selectors and classifiers\n",
    "    combinations = list(product(considered_assessors, considered_selectors, considered_classifiers))\n",
    "\n",
    "    # Initialize the binary win matrix\n",
    "    win_matrix = np.zeros((len(combinations), len(combinations)))\n",
    "\n",
    "    # Iterate through each combination\n",
    "    for i, (assessor1, selector1, classifier1) in enumerate(combinations):\n",
    "        assert assessor1 in filtered_results['assessor'].unique(), f\"Assessor {assessor1} not in the results\"\n",
    "        assert selector1 in filtered_results['selector'].unique(), f\"Selector {selector1} not in the results\"\n",
    "        assert classifier1 in filtered_results[\n",
    "            'predictive_method'].unique(), f\"Classifier {classifier1} not in the results\"\n",
    "        for j in range(i + 1, len(combinations)):\n",
    "            assessor2, selector2, classifier2 = combinations[j]\n",
    "\n",
    "            # Filter for the specific combinations\n",
    "            combo1_results = filtered_results[\n",
    "                (filtered_results['assessor'] == assessor1) & (filtered_results['selector'] == selector1) & (\n",
    "                            filtered_results['predictive_method'] == classifier1)]\n",
    "            combo2_results = filtered_results[\n",
    "                (filtered_results['assessor'] == assessor2) & (filtered_results['selector'] == selector2) & (\n",
    "                            filtered_results['predictive_method'] == classifier2)]\n",
    "\n",
    "            # Compare the metric scores and update the win matrix\n",
    "            if not combo1_results.empty and not combo2_results.empty:\n",
    "                if combo1_results[metric].values[0] > combo2_results[metric].values[0]:\n",
    "                    win_matrix[i, j] = 1\n",
    "                    win_matrix[j, i] = 0\n",
    "                else:\n",
    "                    win_matrix[i, j] = 0\n",
    "                    win_matrix[j, i] = 1\n",
    "\n",
    "    # check if the right number of comparisons were made\n",
    "    assert np.sum(win_matrix) == len(combinations) * (len(combinations) - 1) / 2, \"Not all comparisons were made\"\n",
    "\n",
    "    # now compute the average of the win matrix over axis 1, which leads to a 1d array with the average win rate of that combination over all others:\n",
    "    average_win_rate = np.sum(win_matrix, axis=1) / (len(combinations) - 1)\n",
    "\n",
    "    # create now a pandas series with the average win rate and the corresponding assessor-selector combination\n",
    "    combinations_string = [f\"{assessor}___{selector}___{classifier}\" for assessor, selector, classifier in combinations]\n",
    "    average_win_rate_series = pd.Series(average_win_rate, index=combinations_string)\n",
    "\n",
    "    return average_win_rate_series"
   ],
   "id": "c39ad6b4a2af22ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "5abc1aff4cd86f43"
   },
   "cell_type": "markdown",
   "source": [
    "Test the above"
   ],
   "id": "5abc1aff4cd86f43"
  },
  {
   "metadata": {
    "id": "73413e8289ac450"
   },
   "cell_type": "code",
   "source": [
    "considered_classifiers = [x[2] for x in predictive_method_list]"
   ],
   "id": "73413e8289ac450",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "337e771201d29f15"
   },
   "cell_type": "code",
   "source": [
    "x = compute_win_rate_single_llm_split_dataset(results_reference_reasoning, [\"concatenate_ref_success\", \"concatenate_ref_similarity\"], [\"random\", \"random_best_of\"], considered_classifiers, 'gpt-3.5-turbo-0125', False, metric=\"AUROC\", features=\"openai\")"
   ],
   "id": "337e771201d29f15",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "results_reference_reasoning.subset.unique()"
   ],
   "metadata": {
    "id": "5J98HyWu_h6V",
    "outputId": "99f0b570-42f2-4414-c227-f7b92a53342b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "5J98HyWu_h6V",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = \"gpt-4-0613\"\n",
    "split =  \"OOD_1\"\n",
    "features= \"openai\"\n",
    "\n",
    "results = results_reference_reasoning\n",
    "results_val = results[results['subset'] == \"validation\"]\n",
    "results_test = results[results['subset'] == \"test\"]"
   ],
   "metadata": {
    "id": "sY3uNLdN_p1W"
   },
   "id": "sY3uNLdN_p1W",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "results_val.llm.unique()"
   ],
   "metadata": {
    "id": "B0KHmVXfAIff",
    "outputId": "56b42a5e-ca83-4e80-bab0-f6d787536801",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "B0KHmVXfAIff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "results_test.llm.unique()"
   ],
   "metadata": {
    "id": "dxJ8UviRAft0",
    "outputId": "69c4be80-9302-4fee-94f7-68dd52a200c9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "id": "dxJ8UviRAft0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a06e3ddecbd855b8",
    "outputId": "e6d54ffe-b86c-4907-f6c0-e2a37565b8f0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "x.sort_values(ascending=False)"
   ],
   "id": "a06e3ddecbd855b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "90a924be131535c1"
   },
   "cell_type": "code",
   "source": [
    "# now I need to make a function that averages that over multiple llms, splits and datasets\n",
    "def compute_win_rate_multiple_llm_split_dataset(results_list_dataset, dataset_names, considered_llm_list, considered_split_list, considered_assessors, considered_selectors, considered_classifiers, metric=\"AUROC\", features=\"openai\"):\n",
    "\n",
    "    # create a pandas dataframe where to store the various results\n",
    "    all_win_rates = pd.DataFrame()\n",
    "\n",
    "    for dataset_name, results, considered_llm, considered_split in zip(dataset_names, results_list_dataset, considered_llm_list, considered_split_list):\n",
    "        for llm in considered_llm:\n",
    "            for split in considered_split:\n",
    "                single_win_rate = compute_win_rate_single_llm_split_dataset(results, considered_assessors, considered_selectors, considered_classifiers, llm, split, metric, features)\n",
    "                # append as a new column to the dataframe, identified by the dataset name, llm and split\n",
    "                all_win_rates[f\"{dataset_name}_{split}_{llm}\"] = single_win_rate\n",
    "\n",
    "    return all_win_rates\n"
   ],
   "id": "90a924be131535c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "72dc91ca83c42c65"
   },
   "cell_type": "code",
   "source": [
    "def compute_average_win_rate_from_computed_values(all_win_rates, dataset_names, considered_llm_list, considered_split_list, recursive_average=True):\n",
    "    dataset_averages_df = pd.DataFrame()\n",
    "\n",
    "    cols = []\n",
    "    for dataset_name, considered_llm, considered_split in zip(dataset_names, considered_llm_list, considered_split_list):\n",
    "        cols_dataset = []\n",
    "        for llm in considered_llm:\n",
    "            for split in considered_split:\n",
    "                cols_dataset.append(f\"{dataset_name}_{split}_{llm}\")\n",
    "                cols.append(f\"{dataset_name}_{split}_{llm}\")\n",
    "\n",
    "        dataset_averages_df[dataset_name] = all_win_rates[cols_dataset].mean(axis=1)\n",
    "\n",
    "    if recursive_average:\n",
    "        return dataset_averages_df.mean(axis=1)\n",
    "    else:\n",
    "        return all_win_rates[cols].mean(axis=1)\n"
   ],
   "id": "72dc91ca83c42c65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b751188835fb499b"
   },
   "cell_type": "markdown",
   "source": [
    "**Notice the average is done in two steps: first I average over all considered splits and LLMs for a given dataset, and then I average over all datasets.** This is to avoid a dataset with more splits and LLMs to have more weight in the final average."
   ],
   "id": "b751188835fb499b"
  },
  {
   "metadata": {
    "id": "15aa842bb4db3918"
   },
   "cell_type": "markdown",
   "source": [
    "I will select the best combination of assessor-selector-classifier for each dataset and split by considering the average win rate over all others. In particular, I do this 4 separate instances: \n",
    "- the first to select the best of the generic assessor setups (hence considering all possible concatenations of features, selectors and classifiers)\n",
    "- the second to consider the best reference_all baseline (hence considering only that as assessor and all possible combinations of selectors and classifiers)\n",
    "- the third for the train_all baseline (hence considering only that and one possible choice of selector, as it does not use the selector information).\n",
    "Notice that I also trained a `calibrate_general_classifier` which takes the baseline_all and recalibrates it using test data; however recalibrating does not change the AUC, so it will not be considered in the selection.\n",
    "- Finally, I also consider a baseline where only the random selector is used, but for which the various assessor methods are all used."
   ],
   "id": "15aa842bb4db3918"
  },
  {
   "metadata": {
    "id": "b7c38bc0d9b7fd0e"
   },
   "cell_type": "code",
   "source": [
    "considered_selectors = selector_methods\n",
    "random_selector = [\"random\"]"
   ],
   "id": "b7c38bc0d9b7fd0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d073f58b6d2f420b"
   },
   "cell_type": "code",
   "source": [
    "considered_assessors_no_baseline = [\n",
    "    \"concatenate_ref_success\",\n",
    "    \"concatenate_ref_similarity\",\n",
    "    \"concatenate_ref_similarity_partial_interaction\",\n",
    "]\n",
    "baseline_reference_only = [\"baseline_reference_only\"]\n",
    "baseline_all_train_llms = [\"baseline_all_train_llms\"]"
   ],
   "id": "d073f58b6d2f420b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cc511cb8fb4dd10b"
   },
   "cell_type": "markdown",
   "source": "Compute all win rates for the considered methods and for the baselines.",
   "id": "cc511cb8fb4dd10b"
  },
  {
   "metadata": {
    "id": "3ab359f2f607e897"
   },
   "cell_type": "code",
   "source": [
    "win_rates_no_baseline = compute_win_rate_multiple_llm_split_dataset([results_reference_reasoning, results_reference_helm], [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [split_reasoning, split_helm], considered_assessors_no_baseline, considered_selectors, considered_classifiers)"
   ],
   "id": "3ab359f2f607e897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9229ec51477b57a3"
   },
   "cell_type": "code",
   "source": "win_rates_baseline_reference_only = compute_win_rate_multiple_llm_split_dataset([results_reference_reasoning, results_reference_helm], [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [split_reasoning, split_helm], baseline_reference_only, considered_selectors, considered_classifiers)",
   "id": "9229ec51477b57a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "win_rates_baseline_all_train_llms = compute_win_rate_multiple_llm_split_dataset([results_reference_reasoning, results_reference_helm], [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [split_reasoning, split_helm], baseline_all_train_llms, [considered_selectors[0]], considered_classifiers)  # only consider one selector for sake of efficiency, as they do not impact the AUC ",
   "id": "588e97a9c40cdd1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "win_rates_random_selector = compute_win_rate_multiple_llm_split_dataset([results_reference_reasoning, results_reference_helm], [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [split_reasoning, split_helm], considered_assessors_no_baseline, random_selector, considered_classifiers)",
   "id": "b00c7e0558f4fba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cc8ff714fbe1beb8"
   },
   "cell_type": "markdown",
   "source": [
    "Now, for each dataset and split, I will select the best (selector, assessor setup, classifier) by considering the average win on validation data and LLMs. I will also do the same by restricting to the reference_only assessor setup, to determine the best classifier and selector. The I will have a best setup for each dataset and split, and a best setup for each dataset and split with the reference_only assessor setup."
   ],
   "id": "cc8ff714fbe1beb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_best_method(average_win_rates, dataset_name, split):\n",
    "    \"\"\"\n",
    "    Function to get the best method based on average win rates.\n",
    "\n",
    "    Parameters:\n",
    "    average_win_rates (pd.Series): A pandas series with average win rates.\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    split (str): The split of the dataset.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with the best method and its details.\n",
    "    \"\"\"\n",
    "    # Get the index of the maximum average win rate\n",
    "    best_method = average_win_rates.idxmax()\n",
    "\n",
    "    # Extract the assessor, selector and classifier from the index\n",
    "    assessor, selector, classifier = best_method.split(\"___\")\n",
    "\n",
    "    # Create a DataFrame with the new row\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"dataset\": dataset_name,\n",
    "        \"split\": split,\n",
    "        \"assessor\": assessor,\n",
    "        \"selector\": selector,\n",
    "        \"classifier\": classifier,\n",
    "        \"average_win_rate\": average_win_rates.max()\n",
    "    }])\n",
    "\n",
    "    return new_row"
   ],
   "id": "7cbe519d203e2c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "dd8cd10fb448f669",
    "outputId": "b79401d6-19e9-4527-b404-14c39d689c44",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "best_method_no_baseline_df = pd.DataFrame(columns=[\"dataset\", \"split\", \"assessor\", \"selector\", \"classifier\", \"average_win_rate\"])\n",
    "best_method_baseline_reference_only_df = pd.DataFrame(columns=[\"dataset\", \"split\", \"assessor\", \"selector\", \"classifier\", \"average_win_rate\"])\n",
    "best_method_baseline_all_train_llms_df = pd.DataFrame(columns=[\"dataset\", \"split\", \"assessor\", \"selector\", \"classifier\", \"average_win_rate\"])\n",
    "best_method_random_selector_df = pd.DataFrame(columns=[\"dataset\", \"split\", \"assessor\", \"selector\", \"classifier\", \"average_win_rate\"])\n",
    "\n",
    "for dataset_name, splits, llms in zip([\"reasoning\", \"helm\"], [split_reasoning, split_helm], [validation_llms_reasoning, validation_llms_helm]):\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    for split in splits:\n",
    "        average_win_rates_no_baseline = compute_average_win_rate_from_computed_values(win_rates_no_baseline, [dataset_name], [llms], [[split],])\n",
    "        average_win_rates_baseline_reference_only = compute_average_win_rate_from_computed_values(win_rates_baseline_reference_only, [dataset_name], [llms], [[split],])\n",
    "        average_win_rates_baseline_all_train_llms = compute_average_win_rate_from_computed_values(win_rates_baseline_all_train_llms, [dataset_name], [llms], [[split],])\n",
    "        average_win_rates_random_selector = compute_average_win_rate_from_computed_values(win_rates_random_selector, [dataset_name], [llms], [[split],])\n",
    "\n",
    "        # NO BASELINE\n",
    "        new_row_no_baseline = get_best_method(average_win_rates_no_baseline, dataset_name, split)\n",
    "        # concatenate\n",
    "        best_method_no_baseline_df = pd.concat([best_method_no_baseline_df, new_row_no_baseline], ignore_index=True)\n",
    "\n",
    "        # BASELINE REFERENCE ONLY\n",
    "        new_row_baseline_reference_only = get_best_method(average_win_rates_baseline_reference_only, dataset_name, split)\n",
    "        # concatenate the results\n",
    "        best_method_baseline_reference_only_df = pd.concat([best_method_baseline_reference_only_df, new_row_baseline_reference_only], ignore_index=True)\n",
    "        \n",
    "        # BASELINE ALL TRAIN LLMS\n",
    "        new_row_baseline_all_train_llms = get_best_method(average_win_rates_baseline_all_train_llms, dataset_name, split)\n",
    "        # concatenate the results\n",
    "        best_method_baseline_all_train_llms_df = pd.concat([best_method_baseline_all_train_llms_df, new_row_baseline_all_train_llms], ignore_index=True)\n",
    "        \n",
    "        # RANDOM SELECTOR\n",
    "        new_row_random_selector = get_best_method(average_win_rates_random_selector, dataset_name, split)\n",
    "        # concatenate the results\n",
    "        best_method_random_selector_df = pd.concat([best_method_random_selector_df, new_row_random_selector], ignore_index=True)\n",
    "\n",
    "# save these results\n",
    "save_dataframe(\"results/generic_assessors_best_method_no_baseline_df.csv\", best_method_no_baseline_df)\n",
    "save_dataframe(\"results/generic_assessors_best_method_baseline_reference_only_df.csv\", best_method_baseline_reference_only_df)\n",
    "save_dataframe(\"results/generic_assessors_best_method_baseline_all_train_llms_df.csv\", best_method_baseline_all_train_llms_df)\n",
    "save_dataframe(\"results/generic_assessors_best_method_random_selector_df.csv\", best_method_random_selector_df)"
   ],
   "id": "dd8cd10fb448f669",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b1514f0a57e54a41",
    "outputId": "b998f1f8-6ab3-4e26-deb2-0d3790bc14b6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    }
   },
   "cell_type": "code",
   "source": [
    "best_method_no_baseline_df"
   ],
   "id": "b1514f0a57e54a41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "808a61a0f4d62fab"
   },
   "cell_type": "markdown",
   "source": [
    "\n",
    "I will then perform the selection on multiple subsets of test llms, datasets and splits and see how robust it is.\n"
   ],
   "id": "808a61a0f4d62fab"
  },
  {
   "metadata": {
    "id": "e175f1ce706cf436"
   },
   "cell_type": "markdown",
   "source": [
    "### Everything"
   ],
   "id": "e175f1ce706cf436"
  },
  {
   "metadata": {
    "id": "2d0422a4fe2c228b",
    "outputId": "488d228a-f8c9-402f-a328-25a70ea76920",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "average_win_rates_all = compute_average_win_rate_from_computed_values(win_rates_no_baseline, [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [split_reasoning, split_helm])\n",
    "average_win_rates_all.sort_values(ascending=False)"
   ],
   "id": "2d0422a4fe2c228b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "42993e272f299f10"
   },
   "cell_type": "markdown",
   "source": [
    "### KindsOfReasoning and HELM-Lite separately"
   ],
   "id": "42993e272f299f10"
  },
  {
   "metadata": {
    "id": "3e12c26951a77c64",
    "outputId": "3e903659-5fc5-4437-ff7c-16849173bf4e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "average_win_rates_reasoning = compute_average_win_rate_from_computed_values(win_rates_no_baseline, [\"reasoning\"], [validation_llms_reasoning], [split_reasoning])\n",
    "average_win_rates_reasoning.sort_values(ascending=False)"
   ],
   "id": "3e12c26951a77c64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8c1b9ddc1927c9b",
    "outputId": "acd1f8ee-834d-4933-9a50-86770dc5c51a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "average_win_rates_helm = compute_average_win_rate_from_computed_values(win_rates_no_baseline, [\"helm\"], [validation_llms_helm], [split_helm])\n",
    "average_win_rates_helm.sort_values(ascending=False)"
   ],
   "id": "8c1b9ddc1927c9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "4e5c61057d376e71"
   },
   "cell_type": "markdown",
   "source": [
    "### Random and OOD split separately\n",
    "\n",
    "Considering the random split in the selection of the best assessor-selector combination and then seeing how the selection performs on the OOD split is probably the most interesting (and representative of real world) setup.\n"
   ],
   "id": "4e5c61057d376e71"
  },
  {
   "metadata": {
    "id": "ce7b396f133c97b9",
    "outputId": "003998b4-4d0a-4f68-b68c-c4a4cdbae24f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "average_win_rates_random_split = compute_average_win_rate_from_computed_values(win_rates_no_baseline, [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [[split_reasoning[0]], [split_helm[0]]])\n",
    "average_win_rates_random_split.sort_values(ascending=False)"
   ],
   "id": "ce7b396f133c97b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "67c1301110fec57d",
    "outputId": "c24dd888-f8a3-4562-b6d4-bf4d2126e6c0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "average_win_rates_ood_splits = compute_average_win_rate_from_computed_values(win_rates_no_baseline, [\"reasoning\", \"helm\"], [validation_llms_reasoning, validation_llms_helm], [split_reasoning[1:], split_helm[1:]])\n",
    "average_win_rates_ood_splits.sort_values(ascending=False)"
   ],
   "id": "67c1301110fec57d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b279e97df00a273d"
   },
   "cell_type": "markdown",
   "source": [
    "## Plots - all together"
   ],
   "id": "b279e97df00a273d"
  },
  {
   "metadata": {
    "id": "fa72857829617f2d",
    "outputId": "2cfc858d-61db-4e0d-efcd-af328441a33c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "results_reference_reasoning = load_with_conditions(\"results/generic_assessors_reasoning.pkl\")\n",
    "results_reference_helm = load_with_conditions(\"results/generic_assessors_helm.pkl\")\n",
    "results_all_reasoning = load_with_conditions(\"results/specific_assessors_reasoning.pkl\")\n",
    "results_all_helm = load_with_conditions(\"results/specific_assessors_helm.pkl\")\n",
    "best_method_no_baseline_df = load_with_conditions(\"results/generic_assessors_best_method_no_baseline_df.csv\")\n",
    "best_method_baseline_reference_only_df = load_with_conditions(\"results/generic_assessors_best_method_baseline_reference_only_df.csv\")\n",
    "best_method_baseline_all_train_llms_df = load_with_conditions(\"results/generic_assessors_best_method_baseline_all_train_llms_df.csv\")\n",
    "best_method_random_selector_df = load_with_conditions(\"results/generic_assessors_best_method_random_selector_df.csv\")"
   ],
   "id": "fa72857829617f2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_method_no_baseline_df",
   "id": "43b5a25746296c3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert the one above to a latex table:",
   "id": "29791aa9849e201e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dict_assessor = {\n",
    "       'concatenate_ref_similarity': 'Similarity',\n",
    "       'concatenate_ref_similarity_partial_interaction': 'Similarity with interaction',\n",
    "       'concatenate_ref_success': 'Success',\n",
    "}\n",
    "dict_classifier = {'xgboost': \"XGBoost\", 'logistic_regression_l1_c=0.1': \"Logistic Regression L1 C=0.1\", 'logistic_regression_l1_c=1': \"Logistic Regression L1 C=1\"}\n"
   ],
   "id": "2f578ec4d4a8b96f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "latex_df = best_method_no_baseline_df.copy()\n",
    "# format data \n",
    "latex_df.split = latex_df.split.apply(lambda x: x.replace(\"_\", \" \").replace(\"false\", \"In-distribution\"))\n",
    "latex_df.dataset = latex_df.dataset.apply(lambda x: x.replace(\"reasoning\", \"KindsOfReasoning\").replace(\"helm\", \"HELM-Lite\"))\n",
    "latex_df.assessor = latex_df.assessor.apply(lambda x: dict_assessor[x])\n",
    "latex_df.classifier = latex_df.classifier.apply(lambda x: dict_classifier[x])\n",
    "latex_df.selector = latex_df.selector.apply(lambda x: x.replace(\"_\", \" \").capitalize().replace(\"irt\", \"IRT\").replace(\"llm\", \"LLM\"))\n",
    "# create hierarchical index\n",
    "latex_df.set_index(['dataset', 'split'], inplace=True)\n",
    "latex_df.index.name = \"\"\n",
    "latex_df = latex_df.T\n",
    "latex_df.drop(\"average_win_rate\", inplace=True)\n",
    "latex_df.style.to_latex(buf=\"tab/best_combinations.txt\", hrules=True)"
   ],
   "id": "e8d4c690edcffc08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Do two separate latex tables, one per dataset",
   "id": "3ec12a14685e5cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for dataset in [\"reasoning\", \"helm\"]:\n",
    "    latex_df = best_method_no_baseline_df[best_method_no_baseline_df[\"dataset\"] == dataset].copy()\n",
    "    # format data\n",
    "    latex_df.split = latex_df.split.apply(lambda x: x.replace(\"_\", \" \").replace(\"false\", \"In-distribution\"))\n",
    "    latex_df.assessor = latex_df.assessor.apply(lambda x: dict_assessor[x])\n",
    "    latex_df.classifier = latex_df.classifier.apply(lambda x: dict_classifier[x])\n",
    "    latex_df.selector = latex_df.selector.apply(lambda x: x.replace(\"_\", \" \").capitalize().replace(\"irt\", \"IRT\").replace(\"llm\", \"LLM\"))\n",
    "    latex_df.set_index(['split'], inplace=True)\n",
    "    latex_df.index.name = \"\"\n",
    "    latex_df = latex_df.T\n",
    "    latex_df.drop(\"average_win_rate\", inplace=True)\n",
    "    latex_df.drop(\"dataset\", inplace=True)\n",
    "    latex_df.style.to_latex(buf=f\"tab/best_combinations_{dataset}.txt\", hrules=True)    \n",
    "    latex_df.T.style.to_latex(buf=f\"tab/best_combinations_{dataset}_transposed.txt\", hrules=True)    \n",
    "    \n"
   ],
   "id": "871cd9a024854553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "latex_df.T",
   "id": "4c8f76402041644e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_method_random_selector_df",
   "id": "922173b69a4b5143",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_method_baseline_reference_only_df",
   "id": "9fe2cf77b270bf3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_method_baseline_all_train_llms_df",
   "id": "511d162e8cac585f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The combination selected with the random only selector is, in terms of winning rate over the validation LLMs, always worse than the no-baseline one. However in some on the test LLM the AUC is worse with the latter.",
   "id": "82b78192bb7c6a8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`best_method_baseline_df` contains the results with the best assessor and selector for the reference_only baseline. \n",
    "\n",
    "I also need to add \n",
    "1) the best case where random instances are selected\n",
    "2) the results with the train_all baseline -> how to select the best combination there? "
   ],
   "id": "b330786ea9a693e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_all_reasoning.columns",
   "id": "65fa9d54ad487b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_all_reasoning.shape",
   "id": "630b9dfd375e2e5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_reference_reasoning[\"subset\"].unique()",
   "id": "db434ef4301cdb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_reference_reasoning.columns",
   "id": "a6b7bc24bf52bf92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "97794ec9c3e8e9a1"
   },
   "cell_type": "code",
   "source": [
    "split_reasoning = [False, \"OOD_1\", \"OOD_2\", \"OOD_3\", \"OOD_4\"]\n",
    "split_helm = [False, \"OOD_1\", \"OOD_2\", \"OOD_3\"]\n",
    "split_reasoning_plot = [\"Random\", \"OOD 1\", \"OOD 2\", \"OOD 3\", \"OOD 4\"]\n",
    "split_helm_plot = [\"Random\", \"OOD 1\", \"OOD 2\", \"OOD 3\"]"
   ],
   "id": "97794ec9c3e8e9a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "651be4a632186892"
   },
   "cell_type": "markdown",
   "source": [
    "Notice that I now have a best combination for each split and dataset, as I am using the validation data and LLMs to select them."
   ],
   "id": "651be4a632186892"
  },
  {
   "metadata": {
    "id": "488cd616bbf06924",
    "outputId": "8c0e0658-da17-449b-980d-947915a7f583",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    }
   },
   "cell_type": "code",
   "source": [
    "best_method_no_baseline_df"
   ],
   "id": "488cd616bbf06924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_method_random_selector_df",
   "id": "2b1c522a03c1da98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_reference_reasoning_1 = results_reference_reasoning[results_reference_reasoning[\"split\"] == \"OOD_1\"] \n",
    "results_reference_reasoning_1 = results_reference_reasoning_1[results_reference_reasoning_1[\"selector\"] == \"random\"]\n",
    "results_reference_reasoning_1 = results_reference_reasoning_1[results_reference_reasoning_1[\"assessor\"] == \"concatenate_ref_similarity_partial_interaction\"]\n",
    "results_reference_reasoning_1 = results_reference_reasoning_1[results_reference_reasoning_1[\"predictive_method\"] == \"xgboost\"]\n",
    "results_reference_reasoning_1 = results_reference_reasoning_1[results_reference_reasoning_1[\"subset\"] == \"test\"]\n",
    "results_reference_reasoning_1"
   ],
   "id": "153469c6b2e7b6d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "68b758890ccaee4"
   },
   "cell_type": "markdown",
   "source": "Create all plots (this is what I reported in the paper):",
   "id": "68b758890ccaee4"
  },
  {
   "metadata": {
    "id": "9bea011154ac702e",
    "outputId": "192c4886-3166-429b-9270-fd3a58534b68",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    }
   },
   "cell_type": "code",
   "source": [
    "for orientation in [\"h\"]:#, \"v\"]:\n",
    "    for plot_with_baselines in [True, False]:\n",
    "\n",
    "        feature = \"openai\"\n",
    "        \n",
    "        # orientation = \"v\"\n",
    "        # plot_with_baselines = True\n",
    "        \n",
    "        size = 4\n",
    "        other_dim_depends_on_n_LLM = False\n",
    "        add_fig_title = True\n",
    "        \n",
    "        best_method_no_baseline_df[\"split\"] = best_method_no_baseline_df[\"split\"].apply(lambda x: \"false\" if x.lower() == \"false\" else x)\n",
    "        best_method_baseline_reference_only_df[\"split\"] = best_method_baseline_reference_only_df[\"split\"].apply(lambda x: \"false\" if x.lower() == \"false\" else x)\n",
    "        best_method_baseline_all_train_llms_df[\"split\"] = best_method_baseline_all_train_llms_df[\"split\"].apply(lambda x: \"false\" if x.lower() == \"false\" else x)\n",
    "        best_method_random_selector_df[\"split\"] = best_method_random_selector_df[\"split\"].apply(lambda x: \"false\" if x.lower() == \"false\" else x)\n",
    "        \n",
    "        list_best_method_df = [best_method_no_baseline_df, best_method_random_selector_df, best_method_baseline_reference_only_df, best_method_baseline_all_train_llms_df]\n",
    "        list_names = [\"Reference\", \"Random selector\", \"Reference only\", \"All train data\", ]\n",
    "        \n",
    "        if not plot_with_baselines:\n",
    "            list_best_method_df = list_best_method_df[:1]\n",
    "            list_names = list_names[:1]\n",
    "        \n",
    "        # for both reasoning and helm, put the results with the best assessor and selector and those with the full dataset in a single dataframe.\n",
    "        \n",
    "        for results_reference, results_all, splits_list, splits_list_plot, dataset, dataset_name in zip([results_reference_helm, results_reference_reasoning], [results_all_helm, results_all_reasoning], [split_helm, split_reasoning], [split_helm_plot, split_reasoning_plot], [\"HELM-Lite\", \"KindsOfReasoning\"], [\"helm\", \"reasoning\"]):\n",
    "        \n",
    "            # only keep the test results in the reference setup:\n",
    "            results_reference = results_reference[results_reference[\"subset\"] == \"test\"]\n",
    "            \n",
    "            # rename the \"AUROC\" column into \"AUROC_test\"\n",
    "            results_reference = results_reference.rename(columns={\"AUROC\": \"AUROC_test\"})\n",
    "        \n",
    "            # filter the _all to keep only the llms in the reference\n",
    "            results_all = results_all[results_all[\"llm\"].isin(results_reference[\"llm\"].unique())]\n",
    "        \n",
    "            # only keep those where features=feature\n",
    "            results_all = results_all[results_all[\"features\"] == feature]\n",
    "        \n",
    "            # select the best predictive method and drop duplicates, for the \"all\" case\n",
    "            results_all_best_predictive = results_all.groupby([\"llm\", \"features\", \"split\"]).apply(\n",
    "                lambda x: x[x.AUROC_val == x.AUROC_val.max()]).reset_index(drop=True).drop_duplicates(subset=[\"llm\", \"features\", \"split\"])\n",
    "        \n",
    "            # add a column to specify full and reference\n",
    "            # results_reference[\"type\"] = \"reference\"\n",
    "            results_all_best_predictive[\"type\"] = \"Specific assessor\"\n",
    "        \n",
    "            total_df_dataset = results_all_best_predictive\n",
    "        \n",
    "            for i, split in enumerate(splits_list):\n",
    "                if not split: \n",
    "                    split=\"false\"\n",
    "                \n",
    "                results_reference_split = results_reference[results_reference[\"split\"] == split]\n",
    "                # print(len(results_reference_split))\n",
    "                \n",
    "                for best_method_df, name in zip(list_best_method_df, list_names):\n",
    "                    # overwrite the \"type\" in the results dict:\n",
    "                    results_reference_split[\"type\"] = name\n",
    "                        \n",
    "                    best_assessor = best_method_df[(best_method_df[\"dataset\"] == dataset_name) & (best_method_df[\"split\"] == split)][\"assessor\"].values[0]\n",
    "                    best_selector = best_method_df[(best_method_df[\"dataset\"] == dataset_name) & (best_method_df[\"split\"] == split)][\"selector\"].values[0]\n",
    "                    best_classifier = best_method_df[(best_method_df[\"dataset\"] == dataset_name) & (best_method_df[\"split\"] == split)][\"classifier\"].values[0]\n",
    "            \n",
    "                    # filter for the best assessor, selector and classifier\n",
    "                    results_reference_split_best = results_reference_split[\n",
    "                        (results_reference_split[\"assessor\"] == best_assessor) & (results_reference_split[\"selector\"] == best_selector) & (results_reference_split[\"predictive_method\"] == best_classifier)]\n",
    "                   # print(len(results_reference_split))\n",
    "            \n",
    "                    # drop the \"assessor\", \"selector\" and \"classifier\" columns\n",
    "                    results_reference_split_best = results_reference_split_best.drop(columns=[\"assessor\", \"selector\", \"predictive_method\"])\n",
    "            \n",
    "                    # concatenate the two dataframes\n",
    "                    total_df_dataset = pd.concat([total_df_dataset, results_reference_split_best])\n",
    "        \n",
    "            # # add the dataset name\n",
    "            # total_df_dataset[\"dataset\"] = name\n",
    "        \n",
    "            n_splits = len(splits_list)\n",
    "            # create 4 panels one above the other, wide and short\n",
    "            rescaling_factor = 8 / len(total_df_dataset.llm.unique()) if other_dim_depends_on_n_LLM else 1\n",
    "            if orientation == \"h\":\n",
    "                fig, axes = plt.subplots(1, n_splits, figsize=(size / 4 * 1.8 * n_splits, size / rescaling_factor), sharey=True, sharex=True)\n",
    "            else:\n",
    "                fig, axes = plt.subplots(n_splits, 1, figsize=(size / rescaling_factor, size / 4 * 7 / 4 * n_splits), sharex=True, sharey=True)\n",
    "            \n",
    "            # make sure the split name is uniform\n",
    "            total_df_dataset[\"split\"] = total_df_dataset[\"split\"].apply(lambda x: False if x == \"false\" else x) \n",
    "        \n",
    "            for i, split in enumerate(splits_list):\n",
    "                df_split = total_df_dataset[total_df_dataset[\"split\"] == split]\n",
    "                if orientation == \"h\":\n",
    "                    sns.barplot(data=df_split, x=\"AUROC_test\", y=\"llm\", hue=\"type\", ax=axes[i], legend=False, orient=orientation)\n",
    "                else:\n",
    "                    sns.barplot(data=df_split, x=\"llm\", y=\"AUROC_test\", hue=\"type\", ax=axes[i], legend=False, orient=orientation)\n",
    "                axes[i].set_title(f\"Split: {splits_list_plot[i]}\")\n",
    "                if orientation == \"h\":\n",
    "                    axes[i].set_xlim(xmin=0.4)\n",
    "                    axes[i].set_xlabel(\"AUC\")\n",
    "                    # set ticks and labels at 0.5 and 0.75\n",
    "                    axes[i].set_xticks([0.5, 0.75])\n",
    "                else:\n",
    "                    axes[i].set_ylim(ymin=0.4)\n",
    "                    axes[i].set_ylabel(\"AUC\")\n",
    "                    # set ticks and labels at 0.5 and 0.75\n",
    "                    axes[i].set_yticks([0.5, 0.75])\n",
    "            # rotate ticks\n",
    "            if orientation == \"h\":\n",
    "                axes[0].set_yticklabels(axes[0].get_yticklabels(), rotation=0)\n",
    "                axes[0].set_ylabel(\"Test LLM\")\n",
    "            else:\n",
    "                axes[-1].set_xticklabels(axes[-1].get_xticklabels(), rotation=90)\n",
    "                axes[-1].set_xlabel(\"Test LLM\")\n",
    "        \n",
    "            if add_fig_title:\n",
    "                fig.suptitle(f\"{dataset}\")\n",
    "        \n",
    "            # add legend for reasoning:\n",
    "            if dataset == \"HELM-Lite\":\n",
    "                # Get the current color palette\n",
    "                current_palette = sns.color_palette()\n",
    "        \n",
    "                # Get the unique categories in the 'type' column\n",
    "                categories = total_df_dataset['type'].unique()\n",
    "        \n",
    "                # Create a dictionary that maps each category to a color\n",
    "                color_dict = dict(zip(categories, current_palette))\n",
    "        \n",
    "                legend_elements = [mpatches.Patch(color=color_dict['Specific assessor'], label='Specific assessor'),\n",
    "                                   mpatches.Patch(color=color_dict['Reference'], label='Generic assessor')]\n",
    "                if plot_with_baselines:\n",
    "                    legend_elements += [mpatches.Patch(color=color_dict['Random selector'], label='Random selector'),\n",
    "                                        mpatches.Patch(color=color_dict['Reference only'], label='Reference only'),\n",
    "                                        mpatches.Patch(color=color_dict['All train data'], label='All train data')]\n",
    "                # put the legend below the panels if \"h\"\n",
    "                if orientation == \"h\" and other_dim_depends_on_n_LLM:\n",
    "                    fig.subplots_adjust(bottom=0.2)\n",
    "                    fig.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=2,\n",
    "                               handles=legend_elements, title=\"Method\")\n",
    "                else:\n",
    "                    fig.subplots_adjust(right=0.9)\n",
    "                    fig.legend(loc=\"center left\", bbox_to_anchor=(0.91, 0.5), ncol=1, handles=legend_elements, title=\"Method\")\n",
    "            # tight layout\n",
    "            # plt.tight_layout()\n",
    "        \n",
    "            # savefig\n",
    "            plt.savefig(f\"fig/Fig_2_{dataset}_{orientation}_{'baselines' if plot_with_baselines else 'no_baselines'}.pdf\", bbox_inches='tight')\n",
    "            plt.show()\n",
    "            "
   ],
   "id": "9bea011154ac702e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
